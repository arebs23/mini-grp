{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "\n",
    "1. Introduce the data gathering and parsing\n",
    "1. How to create a VIT from the GPT code\n",
    "    1. Change to an encoder\n",
    "    1. Handling the position embedding\n",
    "    1. Adding the text goals\n",
    "    1. Binning the cotinuous values\n",
    "1. Sanitising the data and standardization.\n",
    "1. Adding goal images\n",
    "1. Adding the input masking\n",
    "1. Evaluating the model in sim\n",
    "1. Recording videos of the results for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get full trajectories\n",
    "Instead of a single image we want something that looks more like a sequence, similar to text. \n",
    "For robotics applications our \"language\" is images and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets\n",
    "\n",
    "The data for robotics applications is often more complicated. There are images, actions and text descriptions. Also, the text descriptions is per episode, instead of at each frame, which is common for RL/BC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some of the patching with images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postition Encodings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "initialize(config_path=\"./conf\", job_name=\"test_app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg: batch_size: 64\n",
      "block_size: 32\n",
      "n_patches: 8\n",
      "max_iters: 10000\n",
      "eval_interval: 100\n",
      "learning_rate: 0.0001\n",
      "eval_iters: 200\n",
      "n_embd: 256\n",
      "r_seed: 1337\n",
      "n_head: 16\n",
      "n_blocks: 4\n",
      "dropout: 0.1\n",
      "action_bins: 10\n",
      "image_shape:\n",
      "- 64\n",
      "- 64\n",
      "- 3\n",
      "dataset: EleutherAI/cifarnet\n",
      "trim: 1000000\n",
      "error: false\n",
      "env: absolute_path\n",
      "\n",
      "{'batch_size': 64, 'block_size': 32, 'n_patches': 8, 'max_iters': 10000, 'eval_interval': 100, 'learning_rate': 0.0001, 'eval_iters': 200, 'n_embd': 256, 'r_seed': 1337, 'n_head': 16, 'n_blocks': 4, 'dropout': 0.1, 'action_bins': 10, 'image_shape': [64, 64, 3], 'dataset': 'EleutherAI/cifarnet', 'trim': 1000000, 'error': False, 'env': 'absolute_path'}\n",
      "Using device:  cuda (Quadro RTX 8000)\n",
      "Features: {'img': Image(mode=None, decode=True, id=None), 'label': ClassLabel(names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgberseth\u001b[0m (\u001b[33mreal-lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mila/g/glen.berseth/playground/octo-mini/wandb/run-20240910_165307-rotnwbkb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/real-lab/mini-vit/runs/rotnwbkb' target=\"_blank\">glamorous-cherry-53</a></strong> to <a href='https://wandb.ai/real-lab/mini-vit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/real-lab/mini-vit' target=\"_blank\">https://wandb.ai/real-lab/mini-vit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/real-lab/mini-vit/runs/rotnwbkb' target=\"_blank\">https://wandb.ai/real-lab/mini-vit/runs/rotnwbkb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.207946 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2366172/3511894213.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(data[\"img\"][ix], dtype=torch.float)\n",
      "/tmp/ipykernel_2366172/3511894213.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(data[\"label\"][ix], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.3035, val loss 2.3038\n",
      "step 100: train loss 2.2440, val loss 2.2439\n",
      "step 200: train loss 2.2139, val loss 2.2181\n",
      "step 300: train loss 2.2002, val loss 2.2109\n",
      "step 400: train loss 2.1853, val loss 2.1817\n",
      "step 500: train loss 2.1662, val loss 2.1742\n",
      "step 600: train loss 2.1565, val loss 2.1596\n",
      "step 700: train loss 2.1505, val loss 2.1620\n",
      "step 800: train loss 2.1545, val loss 2.1593\n",
      "step 900: train loss 2.1261, val loss 2.1337\n",
      "step 1000: train loss 2.1257, val loss 2.1271\n",
      "step 1100: train loss 2.0997, val loss 2.1052\n",
      "step 1200: train loss 2.1139, val loss 2.1135\n",
      "step 1300: train loss 2.1004, val loss 2.1131\n",
      "step 1400: train loss 2.0794, val loss 2.0967\n",
      "step 1500: train loss 2.0942, val loss 2.1096\n",
      "step 1600: train loss 2.0579, val loss 2.0581\n",
      "step 1700: train loss 2.0766, val loss 2.0887\n",
      "step 1800: train loss 2.0951, val loss 2.0958\n",
      "step 1900: train loss 2.0732, val loss 2.0819\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import cv2\n",
    "\n",
    "\n",
    "# data loading\n",
    "def get_batch_vit(split, dataset, batch_size):\n",
    "    # generate a small batch of inputs x and targets y\n",
    "    data = dataset['train'] if split == 'train' else dataset['test']\n",
    "    ix = np.random.randint(int(len(data[\"img\"])), size=(batch_size,))\n",
    "    x = torch.tensor(data[\"img\"][ix], dtype=torch.float)\n",
    "    y = torch.tensor(data[\"label\"][ix], dtype=torch.long)\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "def calc_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(model._cfg.eval_iters)\n",
    "        for k in range(model._cfg.eval_iters):\n",
    "            X, Y = get_batch_vit(split, model._dataset, model._cfg.batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_patches_fast(images):\n",
    "    from einops import rearrange\n",
    "    batch_size, channels, height, width = images.shape\n",
    "    patch_size = height // 8 ## n_patches = 8\n",
    "\n",
    "    p = patch_size # P in maths\n",
    "\n",
    "    patches = rearrange(images, 'b (h p1) (w p2) c -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\n",
    "    return patches\n",
    "\n",
    "def calc_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result\n",
    "\n",
    "## This is an encoder head (full attention)\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        # wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) ## Remove masking\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd=n_embd, dropout=dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd=n_embd, dropout=dropout)\n",
    "        self.ffwd = FeedFoward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class VIT(nn.Module):\n",
    "  def __init__(self, dataset, cfg, mlp_ratio=4):\n",
    "    super(VIT, self).__init__()\n",
    "    self._dataset = dataset\n",
    "    self._cfg = cfg\n",
    "    # assert shape[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    # assert shape[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    self.patch_size = (cfg.image_shape[0] / cfg.n_patches, cfg.image_shape[1] / cfg.n_patches)\n",
    "\n",
    "    #Positional embedding\n",
    "    # self.pos_embed = nn.Parameter(torch.tensor(positional_embeddings(n_patches ** 2 + 1, embedding_size)))\n",
    "    # self. pos_embed.requires_grad = False\n",
    "    self.register_buffer('positional_embeddings', calc_positional_embeddings(cfg.n_patches ** 2 + 1, cfg.n_embd), persistent=False)\n",
    "    # self.position_embedding_table = nn.Embedding(n_patches ** 2 + 1, n_embd)\n",
    "    \n",
    "    self.class_tokens = nn.Parameter(torch.rand(1, cfg.n_embd))\n",
    "\n",
    "    self.input_d = int(cfg.image_shape[2] * self.patch_size[0] * self.patch_size[1])\n",
    "\n",
    "    self.lin_map = nn.Linear(self.input_d, cfg.n_embd, bias=False) \n",
    "\n",
    "    # 4) Transformer encoder blocks\n",
    "    self.blocks = nn.ModuleList([Block(cfg.n_embd, cfg.n_head, dropout=self._cfg.dropout) for _ in range(cfg.n_blocks)])\n",
    "\n",
    "    # 5) Classification MLPk\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(cfg.n_embd, cfg.action_bins),\n",
    "        nn.Softmax(dim=-1)\n",
    "    )\n",
    "\n",
    "  def forward(self, images, targets=None):\n",
    "    # Dividing images into patches\n",
    "    n, c, h, w = images.shape\n",
    "    patches = get_patches_fast(images)\n",
    "    \n",
    "    # Running linear layer tokenization\n",
    "    # Map the vector corresponding to each patch to the hidden size dimension\n",
    "    out = self.lin_map(patches)\n",
    "    \n",
    "    # Adding classification token to the tokens\n",
    "    out = torch.cat((self.class_tokens.expand(n, 1, -1), out), dim=1)\n",
    "    \n",
    "    # Adding positional embedding\n",
    "    out = out + self.positional_embeddings.repeat(n, 1, 1)\n",
    "    # pos_emb = self.position_embedding_table(torch.arange(n_patches ** 2 + 1, device=device)) # (T,C)\n",
    "    # out = out + pos_emb\n",
    "    \n",
    "    # Transformer Blocks\n",
    "    for block in self.blocks:\n",
    "        out = block(out)\n",
    "\n",
    "    # Getting the classification token only\n",
    "    out = out[:, 0]\n",
    "    logits = self.mlp(out)\n",
    "        \n",
    "    if targets is None:\n",
    "        loss = None\n",
    "    else:\n",
    "        # B,T,C = 4,8,2 # batch, time, channels\n",
    "        B, C = logits.shape\n",
    "        # logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "    return (logits, loss)\n",
    "\n",
    "# import hydra, json\n",
    "\n",
    "# @hydra.main(config_path=\"conf\", config_name=\"vit-64\")\n",
    "def my_main():\n",
    "    \n",
    "    from hydra import compose, initialize\n",
    "    from omegaconf import DictConfig, OmegaConf\n",
    "    # initialize(config_path=\"./conf\", job_name=\"test_app\")\n",
    "    cfg = compose(config_name=\"vit-64\", overrides=[\"+env=absolute_path\"])\n",
    "\n",
    "    torch.manual_seed(cfg.r_seed)\n",
    "    print (\"cfg:\", OmegaConf.to_yaml(cfg))\n",
    "    # print (vars(cfg))\n",
    "    print (OmegaConf.to_container(cfg))\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "\n",
    "    from datasets import load_dataset\n",
    "    ds = load_dataset(cfg.dataset)\n",
    "\n",
    "    print('Features:', ds[\"train\"].features)\n",
    "    # np.reshape(np.array(x[\"img\"][i].getdata(), dtype=np.float32)\n",
    "    trim = 1000000 ## Lets see how little data is needed to still get good performance. 1000 is not enough.\n",
    "    dataset = {}\n",
    "    dataset[\"train\"]= {\n",
    "            \"img\": torch.tensor(np.array(ds[\"train\"][\"img\"][:trim], dtype=np.uint8)).to(device),\n",
    "            \"label\": torch.tensor(np.array(ds[\"train\"][\"label\"][:trim], dtype=np.uint8)).to(device) \n",
    "            }         \n",
    "    dataset[\"test\"]=  {\n",
    "            \"img\": torch.tensor(np.array(ds[\"test\"][\"img\"][:trim], dtype=np.uint8)).to(device),\n",
    "            \"label\": torch.tensor(np.array(ds[\"test\"][\"label\"][:trim], dtype=np.uint8)).to(device)\n",
    "            }\n",
    "    # print (\"Results:\", results)\n",
    "    import wandb\n",
    "    # start a new wandb run to track this script\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"mini-vit\",\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config= OmegaConf.to_container(cfg)\n",
    "    )\n",
    "    wandb.run.log_code(\".\")\n",
    "    model = VIT(dataset, cfg)\n",
    "    m = model.to(device)\n",
    "    # print the number of parameters in the model\n",
    "    print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
    "\n",
    "    for iter in range(cfg.max_iters):\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % cfg.eval_interval == 0 or iter == cfg.max_iters - 1:\n",
    "            losses = estimate_loss(model)\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            wandb.log({\"train loss\": losses['train'], \"val loss\": losses['val']})\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch_vit('train', dataset, cfg.batch_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # generate from the model\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    # print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
    "    wandb.finish()\n",
    "    return losses['val']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    results = my_main()\n",
    "    print(\"results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roble",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
