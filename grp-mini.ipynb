{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "\n",
    "1. Introduce the data gathering and parsing\n",
    "1. How to create a VIT from the GPT code\n",
    "    1. Change to an encoder\n",
    "    1. Handling the position embedding\n",
    "    1. Adding the text goals\n",
    "    1. Switching to regresion instead of classification\n",
    "1. Sanitising the data and standardization.\n",
    "1. Adding goal images\n",
    "1. Adding the input masking\n",
    "1. Evaluating the model in sim\n",
    "1. Recording videos of the results for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get full trajectories\n",
    "Instead of a single image we want something that looks more like a sequence, similar to text. \n",
    "For robotics applications our \"language\" is images and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets\n",
    "\n",
    "The data for robotics applications is often more complicated. There are images, actions and text descriptions. Also, the text descriptions is per episode, instead of at each frame, which is common for RL/BC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'roble (Python 3.10.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/mila/g/glen.berseth/.conda/envs/roble ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Install repo\n",
    "!pip3 install -e .\n",
    "!pip3 install -r requirements.txt\n",
    "!pip install moviepy>=1.0.3\n",
    "import cv2\n",
    "# import jax\n",
    "# import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm\n",
    "import rlds, numpy as np\n",
    "import mediapy as media\n",
    "from PIL import Image\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'roble (Python 3.10.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/mila/g/glen.berseth/.conda/envs/roble ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from IPython import display\n",
    "def as_gif(images, path='temp.gif'):\n",
    "  # Render the images as the gif:\n",
    "  images = [Image.fromarray(image) for image in images]\n",
    "  images[0].save(path, save_all=True, append_images=images[1:], duration=1000, loop=0)\n",
    "  gif_bytes = open(path,'rb').read()\n",
    "  return gif_bytes\n",
    "\n",
    "# create RLDS dataset builder\n",
    "builder = tfds.builder_from_directory(builder_dir='gs://gresearch/robotics/bridge/0.1.0/')\n",
    "dataset = builder.as_dataset(split='train[:1]')\n",
    "\n",
    "# sample episode + resize to 256x256 (default third-person cam resolution)\n",
    "episode = next(iter(dataset))\n",
    "print(episode)\n",
    "\n",
    "steps = list(episode['steps'])\n",
    "images = np.array([cv2.resize(np.array(step['observation']['image']), (256, 256)) for step in steps])\n",
    "\n",
    "# extract goal image & language instruction\n",
    "goal_image = images[-1]\n",
    "language_instruction = steps[0]['observation']['natural_language_instruction'].numpy().decode()\n",
    "\n",
    "# visualize episode\n",
    "print(f'Instruction: {language_instruction}')\n",
    "# media.show_video(images, fps=10)\n",
    "display.Image(as_gif(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'roble (Python 3.10.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/mila/g/glen.berseth/.conda/envs/roble ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## Grab a chunk of data for training\n",
    "## Model hyperparameters\n",
    "image_shape = [64, 64, 3]\n",
    "num_episodes = 5 ## How many episodes to grab from the dataset for training\n",
    "\n",
    "builder = tfds.builder_from_directory(builder_dir='gs://gresearch/robotics/bridge/0.1.0/')\n",
    "datasetRemote = builder.as_dataset(split='train[:' + str(num_episodes) + ']')\n",
    "dataset_tmp = {\"img\": [], \"action\": [], \"goal\": [], \"goal_img\": [],\n",
    "                \"rotation_delta\": [], \"open_gripper\": [] }\n",
    "shortest_goal_txt = 10000000000\n",
    "for episode in datasetRemote:\n",
    "    episode_ = {'steps': [] }\n",
    "    episode = list(episode['steps'])\n",
    "    goal_img = cv2.resize(np.array(episode[-1]['observation']['image'], dtype=np.float32), (image_shape[0], image_shape[1]))  \n",
    "    for i in range(len(episode)): ## Resize images to reduce computation\n",
    "        obs = cv2.resize(np.array(episode[i]['observation']['image'], dtype=np.float32), (image_shape[0], image_shape[1])) \n",
    "        goal = episode[i]['observation']['natural_language_instruction'].numpy().decode()\n",
    "        # action = torch.as_tensor(action) # grab first dimention\n",
    "        dataset_tmp[\"img\"].append(obs)\n",
    "        dataset_tmp[\"action\"].append(np.array(episode[i]['action']['world_vector']))\n",
    "        dataset_tmp[\"rotation_delta\"].append(np.array(episode[i]['action']['rotation_delta']))\n",
    "        dataset_tmp[\"open_gripper\"].append(np.array(episode[i]['action']['open_gripper']))\n",
    "        dataset_tmp[\"goal\"].append(goal)\n",
    "        dataset_tmp[\"goal_img\"].append(goal_img)\n",
    "        if len(goal) < shortest_goal_txt: shortest_goal_txt = len(goal)\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set([item for row in dataset_tmp[\"goal\"] for item in row]))) ## Flatten to a long string\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode_txt = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode_txy = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "print(\"vocab_size:\", vocab_size)\n",
    "print(\"example text encode:\", encode_txt(dataset_tmp[\"goal\"][0]))\n",
    "\n",
    "print(\"Dataset shape:\", len(dataset_tmp[\"img\"]))\n",
    "dataset_tmp[\"img\"] = np.array(dataset_tmp[\"img\"], dtype=np.uint8)\n",
    "dataset_tmp[\"action\"] = np.array(dataset_tmp[\"action\"], dtype=np.float32)\n",
    "# dataset_tmp[\"goal\"] = np.array(dataset_tmp[\"goal\"], dtype=np.float32)\n",
    "dataset_tmp[\"goal_img\"] = np.array(dataset_tmp[\"goal_img\"], dtype=np.uint8)\n",
    "\n",
    "dataset = {\"train\": dataset_tmp} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a VIT from the Transformer (NanoGPT) code\n",
    "We have some familiarity with a robotics dataset. It contains multiple types of inputs, text and images and outputs, continuous values We need to make a model that will process in the input data and output the correct values depending on the input observation (img) and goal (text or image). I am going to extend some of Karpathi's gpt-nano code to keep as much of the model and details visable as we learn about this process. We will start with creating a vision transformer from the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'roble (Python 3.10.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/mila/g/glen.berseth/.conda/envs/roble ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## Need to adjust the masking. \n",
    "# We can have fully connected attention (encoder) or partially attended transformers. \n",
    "# Need to discuss this because we are going to modify this a couple times for our GRP.\n",
    "# For images we want an encoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "# Self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) ## Here is what I am interested in\n",
    "query = nn.Linear(C, head_size, bias=False) ## This is what I have\n",
    "value = nn.Linear(C, head_size, bias=False) ## Here is what I will communicate if you find me useful\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T,T))\n",
    "# wei = wei.masked_fill(tril == 0, float('-inf')) ## \n",
    "wei = F.softmax(wei, dim=-1) ## This normalizes the values for a good distribution (sum to 1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'roble (Python 3.10.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/mila/g/glen.berseth/.conda/envs/roble ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## Look at the attension for one head\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Images\n",
    "Transformers process tokens. The nano GPT model was designed to treat each letter as an individual token and train a model to continue outputing more tokens given a recent context of tokens. However most LLMs break up words into phonemes and pieces of words in a particular way to both represent the possible components of words well and keep the number of possible tokens within a reasonable number. This asks the question, how to tokenize an image? We litterally chop the image into $m$ equally sized __patches__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'roble (Python 3.10.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/mila/g/glen.berseth/.conda/envs/roble ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def get_patches(images):\n",
    "  print(\"images.shape:\", images.shape)\n",
    "  batch_size, height, width, channels = images.shape\n",
    "  n_patches = 8\n",
    "  assert height == width, \"only square images are supported\"\n",
    "\n",
    "  patches = torch.zeros(batch_size, n_patches ** 2, height * width * channels // (n_patches ** 2))\n",
    "  patch_size = height // n_patches\n",
    "\n",
    "  for idx, image in enumerate(images):\n",
    "      for row in range(n_patches):\n",
    "          for column in range(n_patches):\n",
    "            ## Channel first\n",
    "            patch = image[:, row * patch_size: (row + 1) * patch_size, column * patch_size: (column + 1) * patch_size]\n",
    "            patches[idx, row * n_patches + column] = patch.flatten()\n",
    "\n",
    "  return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'roble (Python 3.10.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/mila/g/glen.berseth/.conda/envs/roble ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "out = get_patches(torch.tensor(dataset_tmp[\"img\"][:5]))\n",
    "out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some of the patching with images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postition Encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'roble (Python 3.10.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/mila/g/glen.berseth/.conda/envs/roble ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def compute_pos_embed(sequence_length, d):\n",
    "    out = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            out[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'roble (Python 3.10.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/mila/g/glen.berseth/.conda/envs/roble ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def get_patches_fast(images):\n",
    "    from einops import rearrange\n",
    "    batch_size, channels, height, width = images.shape\n",
    "    patch_size = height // n_patches\n",
    "\n",
    "    p = patch_size # P in maths\n",
    "\n",
    "    patches = rearrange(images, 'b (h p1) (w p2) c -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'roble (Python 3.10.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/mila/g/glen.berseth/.conda/envs/roble ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## Code for a Transfomer.\n",
    "n_patches = 8\n",
    "n_embed = 64\n",
    "dropout = 0.1\n",
    "batch_size=64\n",
    "# data loading\n",
    "def get_batch_vit(split):\n",
    "    # generate a small batch of inputs x and targets y\n",
    "    data = dataset['train'] if split == 'train' else dataset['test']\n",
    "    ix = np.random.randint(int(len(data[\"img\"])), size=(batch_size,))\n",
    "    x = torch.tensor(data[\"img\"][ix], dtype=torch.float)\n",
    "    y = torch.tensor(data[\"label\"][ix], dtype=torch.long)\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "def calc_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result\n",
    "\n",
    "def get_patches_fast(images):\n",
    "    from einops import rearrange\n",
    "    batch_size, channels, height, width = images.shape\n",
    "    patch_size = height // n_patches\n",
    "\n",
    "    p = patch_size # P in maths\n",
    "\n",
    "    patches = rearrange(images, 'b (h p1) (w p2) c -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\n",
    "    return patches\n",
    "\n",
    "def calc_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result\n",
    "\n",
    "## This is an encoder head (full attention)\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        # wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) ## Remove masking\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'roble (Python 3.10.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/mila/g/glen.berseth/.conda/envs/roble ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "n_embd = 64\n",
    "action_bins = 10\n",
    "## Creating the model for a Vision Transformers\n",
    "class VIT(nn.Module):\n",
    "  def __init__(self, mlp_ratio=4):\n",
    "    super(VIT, self).__init__()\n",
    "    self.patch_size = (image_shape[0] / n_patches, image_shape[1] / n_patches)\n",
    "\n",
    "    #Positional embedding\n",
    "    self.register_buffer('positional_embeddings', calc_positional_embeddings(n_patches ** 2 + 1, n_embd), persistent=False)\n",
    "    \n",
    "    ## Add class tokens\n",
    "    self.class_tokens = nn.Parameter(torch.rand(1, n_embd))\n",
    "\n",
    "    self.input_d = int(image_shape[2] * self.patch_size[0] * self.patch_size[1])\n",
    "\n",
    "    self.lin_map = nn.Linear(self.input_d, n_embd, bias=False) \n",
    "\n",
    "    # 4) Transformer encoder blocks\n",
    "    self.blocks = nn.ModuleList([Block(n_embd, n_head) for _ in range(n_blocks)])\n",
    "\n",
    "    # 5) Classification MLPk\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(n_embd, action_bins),\n",
    "        nn.Softmax(dim=-1)\n",
    "    )\n",
    "\n",
    "  def forward(self, images, targets=None):\n",
    "    # Dividing images into patches\n",
    "    n, c, h, w = images.shape\n",
    "    patches = get_patches_fast(images)\n",
    "    \n",
    "    # Running linear layer tokenization\n",
    "    # Map the vector corresponding to each patch to the hidden size dimension\n",
    "    out = self.lin_map(patches)\n",
    "    \n",
    "    # Adding classification token to the tokens\n",
    "    out = torch.cat((self.class_tokens.expand(n, 1, -1), out), dim=1)\n",
    "    \n",
    "    # Adding positional embedding\n",
    "    out = out + self.positional_embeddings.repeat(n, 1, 1)\n",
    "    \n",
    "    # Transformer Blocks\n",
    "    for block in self.blocks:\n",
    "        out = block(out)\n",
    "\n",
    "    # Getting the classification token only\n",
    "    out = out[:, 0]\n",
    "    logits = self.mlp(out)\n",
    "        \n",
    "    if targets is None:\n",
    "        loss = None\n",
    "    else:\n",
    "        B, C = logits.shape\n",
    "        targets = targets.view(B)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "    return (logits, loss)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = VIT()\n",
    "    m = model.to(device)\n",
    "    # print the number of parameters in the model\n",
    "    print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    for iter in range(max_iters):\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch_vit('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalist Robotics Policies (GRPs)\n",
    "These models need multiple types of inputs.\n",
    "(goal_text, goal_image, observation_img, robot_post, robot_velocity) -> GRP -> action\n",
    "Let's start with adding text conditioning\n",
    "\n",
    "## Adding text goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'roble (Python 3.10.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/mila/g/glen.berseth/.conda/envs/roble ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## Text goals\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'roble (Python 3.10.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/mila/g/glen.berseth/.conda/envs/roble ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "n_head = 8\n",
    "n_embed = 128\n",
    "\n",
    "class GRP(nn.Module):\n",
    "  def __init__(self, mlp_ratio=4):\n",
    "    super(GRP, self).__init__()\n",
    "    ## Text processing portion\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "    #Positional embedding\n",
    "    self.register_buffer('positional_embeddings', calc_positional_embeddings(n_patches ** 2 + 1, n_embd), persistent=False)\n",
    "\n",
    "    self.patch_size = (image_shape[0] / n_patches, image_shape[1] / n_patches)\n",
    "\n",
    "    self.class_tokens = nn.Parameter(torch.rand(1, n_embd))\n",
    "\n",
    "    self.input_d = int(image_shape[2] * self.patch_size[0] * self.patch_size[1])\n",
    "\n",
    "    self.lin_map = nn.Linear(self.input_d, n_embd, bias=False) \n",
    "    self.lin_map_goal_img = nn.Linear(self.input_d, n_embd, bias=False) \n",
    "\n",
    "    # 4) Transformer encoder blocks\n",
    "    self.blocks = nn.ModuleList([Block(n_embd, n_head) for _ in range(n_blocks)])\n",
    "\n",
    "    # 5) Classification MLPk\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(n_embd, action_bins),\n",
    "        nn.Softmax(dim=-1)\n",
    "    )\n",
    "\n",
    "  def forward(self, images, goals, goal_img, targets):\n",
    "    # Dividing images into patches\n",
    "    n, c, h, w = images.shape\n",
    "    B, T = goals.shape\n",
    "    patches = get_patches_fast(images)\n",
    "    goals_e = self.token_embedding_table(goals)\n",
    "    \n",
    "    # Running linear layer tokenization\n",
    "    # Map the vector corresponding to each patch to the hidden size dimension\n",
    "    out = self.lin_map(patches)\n",
    "    \n",
    "    # Adding classification token to the tokens\n",
    "    out = torch.cat((self.class_tokens.expand(n, 1, -1), out), dim=1)\n",
    "    out = torch.cat([goals_e, out], dim=1) ## Add text and goal image encoding to begining of encoding.\n",
    "    \n",
    "    # Adding positional embedding\n",
    "    out = out + self.positional_embeddings.repeat(n, 1, 1)\n",
    "    \n",
    "    # Transformer Blocks\n",
    "    for block in self.blocks:\n",
    "        out = block(out)\n",
    "\n",
    "    # Getting the classification token only\n",
    "    out = out[:, 0]\n",
    "    logits = self.mlp(out)\n",
    "        \n",
    "    if targets is None:\n",
    "        loss = None\n",
    "    else:\n",
    "        # B,T,C = 4,8,2 # batch, time, channels\n",
    "        B, C = logits.shape\n",
    "        # logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "    return (logits, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'roble (Python 3.10.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/mila/g/glen.berseth/.conda/envs/roble ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## Train this mode to convergence on one batch\n",
    "model = GRP()\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        # wandb.log({\"train loss\": losses['train'], \"val loss\": losses['val']})\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, x2b, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, x2b, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Actions\n",
    "While the above model works so far it is not handeling action properly. Actions can be discrete, and often that helps get a well performing policy, however, the world is continuous and so are most robots. This is just a small change in loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'roble (Python 3.10.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/mila/g/glen.berseth/.conda/envs/roble ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "loss = F.mse_loss(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need to update the model definition\n",
    "n_head = 8\n",
    "n_embed = 128\n",
    "\n",
    "class GRP(nn.Module):\n",
    "  def __init__(self, mlp_ratio=4):\n",
    "    super(GRP, self).__init__()\n",
    "    ## Text processing portion\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "    #Positional embedding\n",
    "    self.register_buffer('positional_embeddings', calc_positional_embeddings(n_patches ** 2 + 1, n_embd), persistent=False)\n",
    "\n",
    "    self.patch_size = (image_shape[0] / n_patches, image_shape[1] / n_patches)\n",
    "\n",
    "    self.class_tokens = nn.Parameter(torch.rand(1, n_embd))\n",
    "\n",
    "    self.input_d = int(image_shape[2] * self.patch_size[0] * self.patch_size[1])\n",
    "\n",
    "    self.lin_map = nn.Linear(self.input_d, n_embd, bias=False) \n",
    "    self.lin_map_goal_img = nn.Linear(self.input_d, n_embd, bias=False) \n",
    "\n",
    "    # 4) Transformer encoder blocks\n",
    "    self.blocks = nn.ModuleList([Block(n_embd, n_head) for _ in range(n_blocks)])\n",
    "\n",
    "    # 5) Classification MLPk\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(n_embd, action_bins),\n",
    "        nn.Softmax(dim=-1)\n",
    "    )\n",
    "\n",
    "  def forward(self, images, goals, goal_img, targets):\n",
    "    # Dividing images into patches\n",
    "    n, c, h, w = images.shape\n",
    "    B, T = goals.shape\n",
    "    patches = get_patches_fast(images)\n",
    "    goals_e = self.token_embedding_table(goals)\n",
    "    \n",
    "    # Running linear layer tokenization\n",
    "    # Map the vector corresponding to each patch to the hidden size dimension\n",
    "    out = self.lin_map(patches)\n",
    "    \n",
    "    # Adding classification token to the tokens\n",
    "    out = torch.cat((self.class_tokens.expand(n, 1, -1), out), dim=1)\n",
    "    out = torch.cat([goals_e, out], dim=1) ## Add text and goal image encoding to begining of encoding.\n",
    "    \n",
    "    # Adding positional embedding\n",
    "    out = out + self.positional_embeddings.repeat(n, 1, 1)\n",
    "    \n",
    "    # Transformer Blocks\n",
    "    for block in self.blocks:\n",
    "        out = block(out)\n",
    "\n",
    "    # Getting the classification token only\n",
    "    out = out[:, 0]\n",
    "    logits = self.mlp(out)\n",
    "        \n",
    "    if targets is None:\n",
    "        loss = None\n",
    "    else:\n",
    "        # B,T,C = 4,8,2 # batch, time, channels\n",
    "        B, C = logits.shape\n",
    "        # logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "    return (logits, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train this mode to convergence on one batch\n",
    "model = GRP()\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        # wandb.log({\"train loss\": losses['train'], \"val loss\": losses['val']})\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, x2b, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, x2b, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roble",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
