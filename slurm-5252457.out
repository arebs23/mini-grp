Date:     Tue Sep  3 11:16:19 AM EDT 2024
Hostname: cn-b005.server.mila.quebec
[=== Module cudatoolkit/12.1.1 loaded ===]
[=== Module miniconda/3 loaded ===]
/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [1e-05, 0.0001] and q=2e-05, but the range is not divisible by `q`. It will be replaced by [1e-05, 9e-05].
  warnings.warn(
[32m[I 2024-09-03 11:16:37,825][0m A new study created in memory with name: grp-64-h-optim[0m
[2024-09-03 11:16:37,826][HYDRA] Study name: grp-64-h-optim
[2024-09-03 11:16:37,826][HYDRA] Storage: None
[2024-09-03 11:16:37,826][HYDRA] Sampler: TPESampler
[2024-09-03 11:16:37,826][HYDRA] Directions: ['minimize']
[2024-09-03 11:16:37,841][HYDRA] Launching 2 jobs locally
[2024-09-03 11:16:37,842][HYDRA] 	#0 : learning_rate=7.000000000000001e-05 dropout=0.2 n_embd=256 batch_size=256
cfg: batch_size: 256
block_size: 32
n_patches: 8
max_iters: 10000
eval_interval: 100
learning_rate: 7.000000000000001e-05
eval_iters: 200
n_embd: 256
r_seed: 1337
n_head: 16
n_blocks: 4
dropout: 0.2
action_bins: 10
image_shape:
- 64
- 64
- 3
dataset: gberseth/mini-bridge-mini64pix
trim: 1000000
error: false

{'batch_size': 256, 'block_size': 32, 'n_patches': 8, 'max_iters': 10000, 'eval_interval': 100, 'learning_rate': 7.000000000000001e-05, 'eval_iters': 200, 'n_embd': 256, 'r_seed': 1337, 'n_head': 16, 'n_blocks': 4, 'dropout': 0.2, 'action_bins': 10, 'image_shape': [64, 64, 3], 'dataset': 'gberseth/mini-bridge-mini64pix', 'trim': 1000000, 'error': False}
Using device:  cuda (Tesla V100-SXM2-32GB)
[2024-09-03 11:16:38,478][datasets][INFO] - PyTorch version 2.4.0 available.
[2024-09-03 11:16:38,481][datasets][INFO] - TensorFlow version 2.15.0 available.
wandb: Currently logged in as: gberseth (real-lab). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /home/mila/g/glen.berseth/playground/octo-mini/multirun/2024-09-03/11-16-34/0/wandb/run-20240903_111735-exnyr2b1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-aardvark-122
wandb: ‚≠êÔ∏è View project at https://wandb.ai/real-lab/mini-grp
wandb: üöÄ View run at https://wandb.ai/real-lab/mini-grp/runs/exnyr2b1
wandb: WARNING No relevant files were detected in the specified directory. No code will be logged to your run.
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(data["img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_goal = torch.tensor(data["goal_img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(data["action"][ix], dtype=torch.long)
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.021 MB of 0.021 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: train loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:   val loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: train loss 2.14683
wandb:   val loss 2.15197
wandb: 
wandb: üöÄ View run royal-aardvark-122 at: https://wandb.ai/real-lab/mini-grp/runs/exnyr2b1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/real-lab/mini-grp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240903_111735-exnyr2b1/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Features: {'img': Image(mode=None, decode=True, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'rotation_delta': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'open_gripper': Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), 'goal': Value(dtype='string', id=None), 'goal_img': Image(mode=None, decode=True, id=None)}
action histogram: [0.11111757 0.11111757 0.11110142 0.11111757 0.13293068 0.08928831
 0.11110142 0.11111757 0.11110142 0.11111757]
bin edges:  [0.  0.9 1.8 2.7 3.6 4.5 5.4 6.3 7.2 8.1 9. ]
Dataset shape: 68766
3.207946 M parameters
step 0: train loss 2.3042, val loss 2.3042
step 100: train loss 2.3015, val loss 2.3020
step 200: train loss 2.2987, val loss 2.2980
step 300: train loss 2.2960, val loss 2.2967
step 400: train loss 2.2956, val loss 2.2956
step 500: train loss 2.2891, val loss 2.2889
step 600: train loss 2.2823, val loss 2.2830
step 700: train loss 2.2815, val loss 2.2821
step 800: train loss 2.2771, val loss 2.2778
step 900: train loss 2.2773, val loss 2.2779
step 1000: train loss 2.2739, val loss 2.2727
step 1100: train loss 2.2700, val loss 2.2733
step 1200: train loss 2.2721, val loss 2.2706
step 1300: train loss 2.2749, val loss 2.2757
step 1400: train loss 2.2676, val loss 2.2680
step 1500: train loss 2.2635, val loss 2.2660
step 1600: train loss 2.2608, val loss 2.2593
step 1700: train loss 2.2510, val loss 2.2511
step 1800: train loss 2.2476, val loss 2.2479
step 1900: train loss 2.2492, val loss 2.2480
step 2000: train loss 2.2418, val loss 2.2393
step 2100: train loss 2.2337, val loss 2.2360
step 2200: train loss 2.2344, val loss 2.2354
step 2300: train loss 2.2369, val loss 2.2392
step 2400: train loss 2.2363, val loss 2.2371
step 2500: train loss 2.2327, val loss 2.2294
step 2600: train loss 2.2276, val loss 2.2263
step 2700: train loss 2.2264, val loss 2.2262
step 2800: train loss 2.2295, val loss 2.2281
step 2900: train loss 2.2194, val loss 2.2210
step 3000: train loss 2.2218, val loss 2.2231
step 3100: train loss 2.2241, val loss 2.2245
step 3200: train loss 2.2171, val loss 2.2178
step 3300: train loss 2.2197, val loss 2.2186
step 3400: train loss 2.2141, val loss 2.2164
step 3500: train loss 2.2137, val loss 2.2150
step 3600: train loss 2.2118, val loss 2.2116
step 3700: train loss 2.2144, val loss 2.2136
step 3800: train loss 2.2103, val loss 2.2121
step 3900: train loss 2.2085, val loss 2.2071
step 4000: train loss 2.2099, val loss 2.2090
step 4100: train loss 2.2086, val loss 2.2083
step 4200: train loss 2.2123, val loss 2.2123
step 4300: train loss 2.2023, val loss 2.2007
step 4400: train loss 2.2144, val loss 2.2157
step 4500: train loss 2.2110, val loss 2.2086
step 4600: train loss 2.2012, val loss 2.1994
step 4700: train loss 2.2043, val loss 2.2018
step 4800: train loss 2.1978, val loss 2.2012
step 4900: train loss 2.1972, val loss 2.2014
step 5000: train loss 2.1988, val loss 2.1965
step 5100: train loss 2.1987, val loss 2.1978
step 5200: train loss 2.2028, val loss 2.2003
step 5300: train loss 2.1953, val loss 2.1964
step 5400: train loss 2.1988, val loss 2.1942
step 5500: train loss 2.1958, val loss 2.1956
step 5600: train loss 2.1992, val loss 2.1984
step 5700: train loss 2.1981, val loss 2.2028
step 5800: train loss 2.1928, val loss 2.1899
step 5900: train loss 2.1963, val loss 2.1961
step 6000: train loss 2.1887, val loss 2.1890
step 6100: train loss 2.1867, val loss 2.1881
step 6200: train loss 2.1824, val loss 2.1794
step 6300: train loss 2.1843, val loss 2.1858
step 6400: train loss 2.1922, val loss 2.1906
step 6500: train loss 2.1848, val loss 2.1822
step 6600: train loss 2.1803, val loss 2.1832
step 6700: train loss 2.1890, val loss 2.1850
step 6800: train loss 2.1778, val loss 2.1829
step 6900: train loss 2.1775, val loss 2.1774
step 7000: train loss 2.1754, val loss 2.1761
step 7100: train loss 2.1775, val loss 2.1781
step 7200: train loss 2.1731, val loss 2.1733
step 7300: train loss 2.1752, val loss 2.1761
step 7400: train loss 2.1762, val loss 2.1775
step 7500: train loss 2.1718, val loss 2.1724
step 7600: train loss 2.1680, val loss 2.1693
step 7700: train loss 2.1766, val loss 2.1767
step 7800: train loss 2.1683, val loss 2.1689
step 7900: train loss 2.1743, val loss 2.1745
step 8000: train loss 2.1664, val loss 2.1675
step 8100: train loss 2.1659, val loss 2.1716
step 8200: train loss 2.1655, val loss 2.1640
step 8300: train loss 2.1643, val loss 2.1616
step 8400: train loss 2.1621, val loss 2.1653
step 8500: train loss 2.1629, val loss 2.1631
step 8600: train loss 2.1648, val loss 2.1694
step 8700: train loss 2.1578, val loss 2.1570
step 8800: train loss 2.1615, val loss 2.1603
step 8900: train loss 2.1585, val loss 2.1569
step 9000: train loss 2.1537, val loss 2.1519
step 9100: train loss 2.1482, val loss 2.1514
step 9200: train loss 2.1572, val loss 2.1529
step 9300: train loss 2.1568, val loss 2.1522
step 9400: train loss 2.1593, val loss 2.1590
step 9500: train loss 2.1539, val loss 2.1498
step 9600: train loss 2.1495, val loss 2.1486
step 9700: train loss 2.1518, val loss 2.1528
step 9800: train loss 2.1472, val loss 2.1525
step 9900: train loss 2.1512, val loss 2.1454
step 9999: train loss 2.1468, val loss 2.1520
[2024-09-03 12:15:46,646][HYDRA] 	#1 : learning_rate=3.0000000000000004e-05 dropout=0.0 n_embd=128 batch_size=512
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /home/mila/g/glen.berseth/playground/octo-mini/multirun/2024-09-03/11-16-34/1/wandb/run-20240903_121634-yzk6m8dd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-snowflake-123
wandb: ‚≠êÔ∏è View project at https://wandb.ai/real-lab/mini-grp
wandb: üöÄ View run at https://wandb.ai/real-lab/mini-grp/runs/yzk6m8dd
wandb: WARNING No relevant files were detected in the specified directory. No code will be logged to your run.
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(data["img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_goal = torch.tensor(data["goal_img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(data["action"][ix], dtype=torch.long)
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: train loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: train loss 2.16951
wandb:   val loss 2.17183
wandb: 
wandb: üöÄ View run dry-snowflake-123 at: https://wandb.ai/real-lab/mini-grp/runs/yzk6m8dd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/real-lab/mini-grp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240903_121634-yzk6m8dd/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
cfg: batch_size: 512
block_size: 32
n_patches: 8
max_iters: 10000
eval_interval: 100
learning_rate: 3.0000000000000004e-05
eval_iters: 200
n_embd: 128
r_seed: 1337
n_head: 16
n_blocks: 4
dropout: 0.0
action_bins: 10
image_shape:
- 64
- 64
- 3
dataset: gberseth/mini-bridge-mini64pix
trim: 1000000
error: false

{'batch_size': 512, 'block_size': 32, 'n_patches': 8, 'max_iters': 10000, 'eval_interval': 100, 'learning_rate': 3.0000000000000004e-05, 'eval_iters': 200, 'n_embd': 128, 'r_seed': 1337, 'n_head': 16, 'n_blocks': 4, 'dropout': 0.0, 'action_bins': 10, 'image_shape': [64, 64, 3], 'dataset': 'gberseth/mini-bridge-mini64pix', 'trim': 1000000, 'error': False}
Using device:  cuda (Tesla V100-SXM2-32GB)
Features: {'img': Image(mode=None, decode=True, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'rotation_delta': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'open_gripper': Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), 'goal': Value(dtype='string', id=None), 'goal_img': Image(mode=None, decode=True, id=None)}
action histogram: [0.11111757 0.11111757 0.11110142 0.11111757 0.13293068 0.08928831
 0.11110142 0.11111757 0.11110142 0.11111757]
bin edges:  [0.  0.9 1.8 2.7 3.6 4.5 5.4 6.3 7.2 8.1 9. ]
Dataset shape: 68766
0.817546 M parameters
step 0: train loss 2.3012, val loss 2.3013
step 100: train loss 2.2986, val loss 2.2988
step 200: train loss 2.2972, val loss 2.2972
step 300: train loss 2.2954, val loss 2.2954
step 400: train loss 2.2948, val loss 2.2944
step 500: train loss 2.2938, val loss 2.2940
step 600: train loss 2.2928, val loss 2.2932
step 700: train loss 2.2913, val loss 2.2915
step 800: train loss 2.2911, val loss 2.2908
step 900: train loss 2.2852, val loss 2.2840
step 1000: train loss 2.2811, val loss 2.2804
step 1100: train loss 2.2799, val loss 2.2792
step 1200: train loss 2.2786, val loss 2.2779
step 1300: train loss 2.2760, val loss 2.2764
step 1400: train loss 2.2728, val loss 2.2738
step 1500: train loss 2.2766, val loss 2.2757
step 1600: train loss 2.2699, val loss 2.2706
step 1700: train loss 2.2692, val loss 2.2695
step 1800: train loss 2.2674, val loss 2.2677
step 1900: train loss 2.2680, val loss 2.2679
step 2000: train loss 2.2665, val loss 2.2661
step 2100: train loss 2.2659, val loss 2.2652
step 2200: train loss 2.2646, val loss 2.2655
step 2300: train loss 2.2622, val loss 2.2605
step 2400: train loss 2.2603, val loss 2.2604
step 2500: train loss 2.2605, val loss 2.2608
step 2600: train loss 2.2555, val loss 2.2566
step 2700: train loss 2.2559, val loss 2.2557
step 2800: train loss 2.2555, val loss 2.2544
step 2900: train loss 2.2510, val loss 2.2518
step 3000: train loss 2.2504, val loss 2.2500
step 3100: train loss 2.2469, val loss 2.2472
step 3200: train loss 2.2463, val loss 2.2472
step 3300: train loss 2.2456, val loss 2.2451
step 3400: train loss 2.2445, val loss 2.2457
step 3500: train loss 2.2431, val loss 2.2452
step 3600: train loss 2.2422, val loss 2.2414
step 3700: train loss 2.2393, val loss 2.2400
step 3800: train loss 2.2417, val loss 2.2411
step 3900: train loss 2.2405, val loss 2.2410
step 4000: train loss 2.2360, val loss 2.2344
step 4100: train loss 2.2375, val loss 2.2360
step 4200: train loss 2.2345, val loss 2.2338
step 4300: train loss 2.2509, val loss 2.2490
step 4400: train loss 2.2325, val loss 2.2324
step 4500: train loss 2.2375, val loss 2.2378
step 4600: train loss 2.2344, val loss 2.2337
step 4700: train loss 2.2293, val loss 2.2283
step 4800: train loss 2.2256, val loss 2.2278
step 4900: train loss 2.2269, val loss 2.2272
step 5000: train loss 2.2291, val loss 2.2312
step 5100: train loss 2.2260, val loss 2.2255
step 5200: train loss 2.2226, val loss 2.2209
step 5300: train loss 2.2206, val loss 2.2220
step 5400: train loss 2.2193, val loss 2.2195
step 5500: train loss 2.2180, val loss 2.2173
step 5600: train loss 2.2192, val loss 2.2176
step 5700: train loss 2.2177, val loss 2.2150
step 5800: train loss 2.2225, val loss 2.2230
step 5900: train loss 2.2128, val loss 2.2137
step 6000: train loss 2.2113, val loss 2.2116
step 6100: train loss 2.2128, val loss 2.2123
step 6200: train loss 2.2193, val loss 2.2198
step 6300: train loss 2.2075, val loss 2.2067
step 6400: train loss 2.2083, val loss 2.2071
step 6500: train loss 2.2077, val loss 2.2064
step 6600: train loss 2.2053, val loss 2.2042
step 6700: train loss 2.2050, val loss 2.2054
step 6800: train loss 2.2064, val loss 2.2043
step 6900: train loss 2.2084, val loss 2.2051
step 7000: train loss 2.2026, val loss 2.2025
step 7100: train loss 2.2089, val loss 2.2104
step 7200: train loss 2.2027, val loss 2.2010
step 7300: train loss 2.1964, val loss 2.1960
step 7400: train loss 2.1966, val loss 2.1980
step 7500: train loss 2.1968, val loss 2.1972
step 7600: train loss 2.1939, val loss 2.1920
step 7700: train loss 2.1956, val loss 2.1946
step 7800: train loss 2.1898, val loss 2.1905
step 7900: train loss 2.1883, val loss 2.1877
step 8000: train loss 2.1893, val loss 2.1888
step 8100: train loss 2.1925, val loss 2.1895
step 8200: train loss 2.1845, val loss 2.1864
step 8300: train loss 2.1874, val loss 2.1862
step 8400: train loss 2.1850, val loss 2.1843
step 8500: train loss 2.1820, val loss 2.1815
step 8600: train loss 2.1817, val loss 2.1824
step 8700: train loss 2.1859, val loss 2.1869
step 8800: train loss 2.1805, val loss 2.1800
step 8900: train loss 2.1743, val loss 2.1767
step 9000: train loss 2.1807, val loss 2.1801
step 9100: train loss 2.1735, val loss 2.1742
step 9200: train loss 2.1878, val loss 2.1868
step 9300: train loss 2.1772, val loss 2.1806
step 9400: train loss 2.1763, val loss 2.1759
step 9500: train loss 2.1731, val loss 2.1728
step 9600: train loss 2.1780, val loss 2.1791
step 9700: train loss 2.1751, val loss 2.1732
step 9800: train loss 2.1710, val loss 2.1700
step 9900: train loss 2.1680, val loss 2.1674
step 9999: train loss 2.1695, val loss 2.1718
[2024-09-03 13:21:49,223][HYDRA] Launching 2 jobs locally
[2024-09-03 13:21:49,223][HYDRA] 	#2 : learning_rate=7.000000000000001e-05 dropout=0.0 n_embd=64 batch_size=512
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /home/mila/g/glen.berseth/playground/octo-mini/multirun/2024-09-03/11-16-34/2/wandb/run-20240903_132241-w8ssdzog
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-pyramid-124
wandb: ‚≠êÔ∏è View project at https://wandb.ai/real-lab/mini-grp
wandb: üöÄ View run at https://wandb.ai/real-lab/mini-grp/runs/w8ssdzog
wandb: WARNING No relevant files were detected in the specified directory. No code will be logged to your run.
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(data["img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_goal = torch.tensor(data["goal_img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(data["action"][ix], dtype=torch.long)
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.020 MB uploadedwandb: | 0.005 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: train loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: train loss 2.18591
wandb:   val loss 2.18593
wandb: 
wandb: üöÄ View run genial-pyramid-124 at: https://wandb.ai/real-lab/mini-grp/runs/w8ssdzog
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/real-lab/mini-grp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240903_132241-w8ssdzog/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
cfg: batch_size: 512
block_size: 32
n_patches: 8
max_iters: 10000
eval_interval: 100
learning_rate: 7.000000000000001e-05
eval_iters: 200
n_embd: 64
r_seed: 1337
n_head: 16
n_blocks: 4
dropout: 0.0
action_bins: 10
image_shape:
- 64
- 64
- 3
dataset: gberseth/mini-bridge-mini64pix
trim: 1000000
error: false

{'batch_size': 512, 'block_size': 32, 'n_patches': 8, 'max_iters': 10000, 'eval_interval': 100, 'learning_rate': 7.000000000000001e-05, 'eval_iters': 200, 'n_embd': 64, 'r_seed': 1337, 'n_head': 16, 'n_blocks': 4, 'dropout': 0.0, 'action_bins': 10, 'image_shape': [64, 64, 3], 'dataset': 'gberseth/mini-bridge-mini64pix', 'trim': 1000000, 'error': False}
Using device:  cuda (Tesla V100-SXM2-32GB)
Features: {'img': Image(mode=None, decode=True, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'rotation_delta': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'open_gripper': Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), 'goal': Value(dtype='string', id=None), 'goal_img': Image(mode=None, decode=True, id=None)}
action histogram: [0.11111757 0.11111757 0.11110142 0.11111757 0.13293068 0.08928831
 0.11110142 0.11111757 0.11110142 0.11111757]
bin edges:  [0.  0.9 1.8 2.7 3.6 4.5 5.4 6.3 7.2 8.1 9. ]
Dataset shape: 68766
0.21217 M parameters
step 0: train loss 2.3037, val loss 2.3038
step 100: train loss 2.2978, val loss 2.2984
step 200: train loss 2.2953, val loss 2.2954
step 300: train loss 2.2939, val loss 2.2936
step 400: train loss 2.2933, val loss 2.2933
step 500: train loss 2.2929, val loss 2.2920
step 600: train loss 2.2913, val loss 2.2912
step 700: train loss 2.2906, val loss 2.2912
step 800: train loss 2.2910, val loss 2.2911
step 900: train loss 2.2869, val loss 2.2873
step 1000: train loss 2.2844, val loss 2.2840
step 1100: train loss 2.2815, val loss 2.2816
step 1200: train loss 2.2767, val loss 2.2769
step 1300: train loss 2.2764, val loss 2.2754
step 1400: train loss 2.2734, val loss 2.2739
step 1500: train loss 2.2726, val loss 2.2732
step 1600: train loss 2.2705, val loss 2.2700
step 1700: train loss 2.2676, val loss 2.2683
step 1800: train loss 2.2674, val loss 2.2670
step 1900: train loss 2.2673, val loss 2.2661
step 2000: train loss 2.2649, val loss 2.2653
step 2100: train loss 2.2677, val loss 2.2684
step 2200: train loss 2.2601, val loss 2.2611
step 2300: train loss 2.2583, val loss 2.2611
step 2400: train loss 2.2580, val loss 2.2590
step 2500: train loss 2.2550, val loss 2.2547
step 2600: train loss 2.2507, val loss 2.2490
step 2700: train loss 2.2460, val loss 2.2464
step 2800: train loss 2.2476, val loss 2.2498
step 2900: train loss 2.2510, val loss 2.2507
step 3000: train loss 2.2413, val loss 2.2421
step 3100: train loss 2.2465, val loss 2.2461
step 3200: train loss 2.2441, val loss 2.2450
step 3300: train loss 2.2351, val loss 2.2372
step 3400: train loss 2.2318, val loss 2.2292
step 3500: train loss 2.2302, val loss 2.2295
step 3600: train loss 2.2279, val loss 2.2293
step 3700: train loss 2.2352, val loss 2.2356
step 3800: train loss 2.2315, val loss 2.2321
step 3900: train loss 2.2266, val loss 2.2244
step 4000: train loss 2.2254, val loss 2.2261
step 4100: train loss 2.2247, val loss 2.2249
step 4200: train loss 2.2247, val loss 2.2250
step 4300: train loss 2.2243, val loss 2.2250
step 4400: train loss 2.2241, val loss 2.2226
step 4500: train loss 2.2216, val loss 2.2207
step 4600: train loss 2.2175, val loss 2.2160
step 4700: train loss 2.2200, val loss 2.2190
step 4800: train loss 2.2163, val loss 2.2170
step 4900: train loss 2.2159, val loss 2.2154
step 5000: train loss 2.2137, val loss 2.2127
step 5100: train loss 2.2155, val loss 2.2150
step 5200: train loss 2.2158, val loss 2.2164
step 5300: train loss 2.2118, val loss 2.2116
step 5400: train loss 2.2157, val loss 2.2155
step 5500: train loss 2.2124, val loss 2.2147
step 5600: train loss 2.2178, val loss 2.2161
step 5700: train loss 2.2132, val loss 2.2137
step 5800: train loss 2.2189, val loss 2.2206
step 5900: train loss 2.2084, val loss 2.2095
step 6000: train loss 2.2094, val loss 2.2102
step 6100: train loss 2.2063, val loss 2.2053
step 6200: train loss 2.2061, val loss 2.2060
step 6300: train loss 2.2050, val loss 2.2055
step 6400: train loss 2.2045, val loss 2.2041
step 6500: train loss 2.2085, val loss 2.2096
step 6600: train loss 2.2045, val loss 2.2028
step 6700: train loss 2.2027, val loss 2.2043
step 6800: train loss 2.2116, val loss 2.2120
step 6900: train loss 2.2091, val loss 2.2089
step 7000: train loss 2.1993, val loss 2.1978
step 7100: train loss 2.2046, val loss 2.2045
step 7200: train loss 2.1988, val loss 2.2007
step 7300: train loss 2.2054, val loss 2.2034
step 7400: train loss 2.2016, val loss 2.2011
step 7500: train loss 2.2005, val loss 2.1992
step 7600: train loss 2.1974, val loss 2.1975
step 7700: train loss 2.2023, val loss 2.2039
step 7800: train loss 2.1970, val loss 2.1961
step 7900: train loss 2.1953, val loss 2.1948
step 8000: train loss 2.1938, val loss 2.1946
step 8100: train loss 2.1925, val loss 2.1931
step 8200: train loss 2.1954, val loss 2.1953
step 8300: train loss 2.1933, val loss 2.1945
step 8400: train loss 2.1997, val loss 2.1974
step 8500: train loss 2.1897, val loss 2.1922
step 8600: train loss 2.1927, val loss 2.1893
step 8700: train loss 2.1918, val loss 2.1910
step 8800: train loss 2.1916, val loss 2.1927
step 8900: train loss 2.2004, val loss 2.2003
step 9000: train loss 2.1920, val loss 2.1943
step 9100: train loss 2.2002, val loss 2.1990
step 9200: train loss 2.1912, val loss 2.1882
step 9300: train loss 2.1898, val loss 2.1889
step 9400: train loss 2.1903, val loss 2.1880
step 9500: train loss 2.1914, val loss 2.1900
step 9600: train loss 2.1905, val loss 2.1907
step 9700: train loss 2.1868, val loss 2.1880
step 9800: train loss 2.1914, val loss 2.1921
step 9900: train loss 2.1908, val loss 2.1893
step 9999: train loss 2.1859, val loss 2.1859
[2024-09-03 14:14:33,710][HYDRA] 	#3 : learning_rate=1e-05 dropout=0.2 n_embd=512 batch_size=256
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /home/mila/g/glen.berseth/playground/octo-mini/multirun/2024-09-03/11-16-34/3/wandb/run-20240903_141521-s7tdhxhw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-wildflower-125
wandb: ‚≠êÔ∏è View project at https://wandb.ai/real-lab/mini-grp
wandb: üöÄ View run at https://wandb.ai/real-lab/mini-grp/runs/s7tdhxhw
wandb: WARNING No relevant files were detected in the specified directory. No code will be logged to your run.
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(data["img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_goal = torch.tensor(data["goal_img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(data["action"][ix], dtype=torch.long)
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: train loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: train loss 2.19876
wandb:   val loss 2.19565
wandb: 
wandb: üöÄ View run deep-wildflower-125 at: https://wandb.ai/real-lab/mini-grp/runs/s7tdhxhw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/real-lab/mini-grp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240903_141521-s7tdhxhw/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
cfg: batch_size: 256
block_size: 32
n_patches: 8
max_iters: 10000
eval_interval: 100
learning_rate: 1.0e-05
eval_iters: 200
n_embd: 512
r_seed: 1337
n_head: 16
n_blocks: 4
dropout: 0.2
action_bins: 10
image_shape:
- 64
- 64
- 3
dataset: gberseth/mini-bridge-mini64pix
trim: 1000000
error: false

{'batch_size': 256, 'block_size': 32, 'n_patches': 8, 'max_iters': 10000, 'eval_interval': 100, 'learning_rate': 1e-05, 'eval_iters': 200, 'n_embd': 512, 'r_seed': 1337, 'n_head': 16, 'n_blocks': 4, 'dropout': 0.2, 'action_bins': 10, 'image_shape': [64, 64, 3], 'dataset': 'gberseth/mini-bridge-mini64pix', 'trim': 1000000, 'error': False}
Using device:  cuda (Tesla V100-SXM2-32GB)
Features: {'img': Image(mode=None, decode=True, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'rotation_delta': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'open_gripper': Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), 'goal': Value(dtype='string', id=None), 'goal_img': Image(mode=None, decode=True, id=None)}
action histogram: [0.11111757 0.11111757 0.11110142 0.11111757 0.13293068 0.08928831
 0.11110142 0.11111757 0.11110142 0.11111757]
bin edges:  [0.  0.9 1.8 2.7 3.6 4.5 5.4 6.3 7.2 8.1 9. ]
Dataset shape: 68766
12.707338 M parameters
step 0: train loss 2.3041, val loss 2.3047
step 100: train loss 2.2982, val loss 2.2989
step 200: train loss 2.2976, val loss 2.2974
step 300: train loss 2.2961, val loss 2.2959
step 400: train loss 2.2964, val loss 2.2958
step 500: train loss 2.2959, val loss 2.2954
step 600: train loss 2.2927, val loss 2.2937
step 700: train loss 2.2939, val loss 2.2926
step 800: train loss 2.2937, val loss 2.2938
step 900: train loss 2.2932, val loss 2.2930
step 1000: train loss 2.2892, val loss 2.2902
step 1100: train loss 2.2890, val loss 2.2889
step 1200: train loss 2.2880, val loss 2.2888
step 1300: train loss 2.2831, val loss 2.2825
step 1400: train loss 2.2820, val loss 2.2820
step 1500: train loss 2.2832, val loss 2.2826
step 1600: train loss 2.2779, val loss 2.2774
step 1700: train loss 2.2767, val loss 2.2755
step 1800: train loss 2.2764, val loss 2.2752
step 1900: train loss 2.2769, val loss 2.2761
step 2000: train loss 2.2739, val loss 2.2741
step 2100: train loss 2.2750, val loss 2.2741
step 2200: train loss 2.2740, val loss 2.2737
step 2300: train loss 2.2720, val loss 2.2721
step 2400: train loss 2.2701, val loss 2.2706
step 2500: train loss 2.2724, val loss 2.2718
step 2600: train loss 2.2695, val loss 2.2691
step 2700: train loss 2.2695, val loss 2.2678
step 2800: train loss 2.2701, val loss 2.2685
step 2900: train loss 2.2679, val loss 2.2666
step 3000: train loss 2.2657, val loss 2.2672
step 3100: train loss 2.2646, val loss 2.2660
step 3200: train loss 2.2647, val loss 2.2646
step 3300: train loss 2.2607, val loss 2.2616
step 3400: train loss 2.2604, val loss 2.2584
step 3500: train loss 2.2577, val loss 2.2589
step 3600: train loss 2.2609, val loss 2.2598
step 3700: train loss 2.2564, val loss 2.2520
step 3800: train loss 2.2490, val loss 2.2498
step 3900: train loss 2.2497, val loss 2.2471
step 4000: train loss 2.2426, val loss 2.2431
step 4100: train loss 2.2416, val loss 2.2414
step 4200: train loss 2.2401, val loss 2.2447
step 4300: train loss 2.2383, val loss 2.2384
step 4400: train loss 2.2406, val loss 2.2434
step 4500: train loss 2.2321, val loss 2.2346
step 4600: train loss 2.2365, val loss 2.2365
step 4700: train loss 2.2400, val loss 2.2428
step 4800: train loss 2.2316, val loss 2.2345
step 4900: train loss 2.2305, val loss 2.2296
step 5000: train loss 2.2311, val loss 2.2312
step 5100: train loss 2.2270, val loss 2.2264
step 5200: train loss 2.2313, val loss 2.2270
step 5300: train loss 2.2255, val loss 2.2267
step 5400: train loss 2.2255, val loss 2.2251
step 5500: train loss 2.2306, val loss 2.2291
step 5600: train loss 2.2295, val loss 2.2299
step 5700: train loss 2.2291, val loss 2.2305
step 5800: train loss 2.2235, val loss 2.2244
step 5900: train loss 2.2283, val loss 2.2270
step 6000: train loss 2.2229, val loss 2.2250
step 6100: train loss 2.2232, val loss 2.2224
step 6200: train loss 2.2233, val loss 2.2208
step 6300: train loss 2.2210, val loss 2.2208
step 6400: train loss 2.2203, val loss 2.2220
step 6500: train loss 2.2228, val loss 2.2236
step 6600: train loss 2.2221, val loss 2.2230
step 6700: train loss 2.2189, val loss 2.2184
step 6800: train loss 2.2174, val loss 2.2190
step 6900: train loss 2.2177, val loss 2.2188
step 7000: train loss 2.2221, val loss 2.2241
step 7100: train loss 2.2190, val loss 2.2183
step 7200: train loss 2.2152, val loss 2.2142
step 7300: train loss 2.2156, val loss 2.2156
step 7400: train loss 2.2155, val loss 2.2137
step 7500: train loss 2.2140, val loss 2.2133
step 7600: train loss 2.2099, val loss 2.2110
step 7700: train loss 2.2115, val loss 2.2129
step 7800: train loss 2.2082, val loss 2.2115
step 7900: train loss 2.2119, val loss 2.2089
step 8000: train loss 2.2109, val loss 2.2139
step 8100: train loss 2.2138, val loss 2.2139
step 8200: train loss 2.2133, val loss 2.2106
step 8300: train loss 2.2123, val loss 2.2109
step 8400: train loss 2.2102, val loss 2.2084
step 8500: train loss 2.2078, val loss 2.2075
step 8600: train loss 2.2065, val loss 2.2102
step 8700: train loss 2.2089, val loss 2.2087
step 8800: train loss 2.2048, val loss 2.2068
step 8900: train loss 2.2058, val loss 2.2057
step 9000: train loss 2.2099, val loss 2.2053
step 9100: train loss 2.2057, val loss 2.2076
step 9200: train loss 2.2049, val loss 2.2052
step 9300: train loss 2.2024, val loss 2.2029
step 9400: train loss 2.2001, val loss 2.2025
step 9500: train loss 2.2046, val loss 2.2039
step 9600: train loss 2.2045, val loss 2.2035
step 9700: train loss 2.2001, val loss 2.1993
step 9800: train loss 2.1978, val loss 2.1988
step 9900: train loss 2.2010, val loss 2.1976
step 9999: train loss 2.1988, val loss 2.1957
[2024-09-03 16:17:56,921][HYDRA] Launching 2 jobs locally
[2024-09-03 16:17:56,921][HYDRA] 	#4 : learning_rate=7.000000000000001e-05 dropout=0.2 n_embd=512 batch_size=512
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /home/mila/g/glen.berseth/playground/octo-mini/multirun/2024-09-03/11-16-34/4/wandb/run-20240903_161850-r4ql6gmt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-glitter-126
wandb: ‚≠êÔ∏è View project at https://wandb.ai/real-lab/mini-grp
wandb: üöÄ View run at https://wandb.ai/real-lab/mini-grp/runs/r4ql6gmt
wandb: WARNING No relevant files were detected in the specified directory. No code will be logged to your run.
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(data["img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_goal = torch.tensor(data["goal_img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(data["action"][ix], dtype=torch.long)
slurmstepd: error: *** JOB 5252457 ON cn-b005 CANCELLED AT 2024-09-03T16:25:48 DUE TO PREEMPTION ***

======== GPU REPORT ========

==============NVSMI LOG==============

Timestamp                                 : Tue Sep  3 16:25:48 2024
Driver Version                            : 535.183.06
CUDA Version                              : 12.2

Attached GPUs                             : 1
GPU 00000000:B3:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 3628222
            GPU Utilization               : 97 %
            Memory Utilization            : 65 %
            Max memory usage              : 20774 MiB
            Time                          : 0 ms
            Is Running                    : 1

Tue Sep  3 16:25:48 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla V100-SXM2-32GB           On  | 00000000:B3:00.0 Off |                    0 |
| N/A   64C    P0             283W / 300W |  20777MiB / 32768MiB |    100%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A   3628222      C   python                                    20774MiB |
+---------------------------------------------------------------------------------------+
Date:     Tue Sep  3 04:32:16 PM EDT 2024
Hostname: cn-c015.server.mila.quebec
[=== Module cudatoolkit/12.1.1 loaded ===]
[=== Module miniconda/3 loaded ===]
/home/mila/g/glen.berseth/.conda/envs/roble/lib/python3.10/site-packages/optuna/distributions.py:545: UserWarning: The distribution is specified by [1e-05, 0.0001] and q=2e-05, but the range is not divisible by `q`. It will be replaced by [1e-05, 9e-05].
  warnings.warn(
[32m[I 2024-09-03 16:32:33,301][0m A new study created in memory with name: grp-64-h-optim[0m
[2024-09-03 16:32:33,301][HYDRA] Study name: grp-64-h-optim
[2024-09-03 16:32:33,301][HYDRA] Storage: None
[2024-09-03 16:32:33,301][HYDRA] Sampler: TPESampler
[2024-09-03 16:32:33,301][HYDRA] Directions: ['minimize']
[2024-09-03 16:32:33,311][HYDRA] Launching 2 jobs locally
[2024-09-03 16:32:33,311][HYDRA] 	#0 : learning_rate=7.000000000000001e-05 dropout=0.2 n_embd=256 batch_size=256
cfg: batch_size: 256
block_size: 32
n_patches: 8
max_iters: 10000
eval_interval: 100
learning_rate: 7.000000000000001e-05
eval_iters: 200
n_embd: 256
r_seed: 1337
n_head: 16
n_blocks: 4
dropout: 0.2
action_bins: 10
image_shape:
- 64
- 64
- 3
dataset: gberseth/mini-bridge-mini64pix
trim: 1000000
error: false

{'batch_size': 256, 'block_size': 32, 'n_patches': 8, 'max_iters': 10000, 'eval_interval': 100, 'learning_rate': 7.000000000000001e-05, 'eval_iters': 200, 'n_embd': 256, 'r_seed': 1337, 'n_head': 16, 'n_blocks': 4, 'dropout': 0.2, 'action_bins': 10, 'image_shape': [64, 64, 3], 'dataset': 'gberseth/mini-bridge-mini64pix', 'trim': 1000000, 'error': False}
Using device:  cuda (Quadro RTX 8000)
[2024-09-03 16:32:33,976][datasets][INFO] - PyTorch version 2.4.0 available.
[2024-09-03 16:32:33,978][datasets][INFO] - TensorFlow version 2.15.0 available.
wandb: Currently logged in as: gberseth (real-lab). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /home/mila/g/glen.berseth/playground/octo-mini/multirun/2024-09-03/16-32-30/0/wandb/run-20240903_163320-7f6mford
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-pine-127
wandb: ‚≠êÔ∏è View project at https://wandb.ai/real-lab/mini-grp
wandb: üöÄ View run at https://wandb.ai/real-lab/mini-grp/runs/7f6mford
wandb: WARNING No relevant files were detected in the specified directory. No code will be logged to your run.
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(data["img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_goal = torch.tensor(data["goal_img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(data["action"][ix], dtype=torch.long)
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.008 MB uploadedwandb: | 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: train loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: train loss 2.1477
wandb:   val loss 2.14985
wandb: 
wandb: üöÄ View run playful-pine-127 at: https://wandb.ai/real-lab/mini-grp/runs/7f6mford
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/real-lab/mini-grp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240903_163320-7f6mford/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Features: {'img': Image(mode=None, decode=True, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'rotation_delta': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'open_gripper': Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), 'goal': Value(dtype='string', id=None), 'goal_img': Image(mode=None, decode=True, id=None)}
action histogram: [0.11111757 0.11111757 0.11110142 0.11111757 0.13293068 0.08928831
 0.11110142 0.11111757 0.11110142 0.11111757]
bin edges:  [0.  0.9 1.8 2.7 3.6 4.5 5.4 6.3 7.2 8.1 9. ]
Dataset shape: 68766
3.207946 M parameters
step 0: train loss 2.3044, val loss 2.3046
step 100: train loss 2.2985, val loss 2.2984
step 200: train loss 2.2956, val loss 2.2949
step 300: train loss 2.2944, val loss 2.2939
step 400: train loss 2.3017, val loss 2.3020
step 500: train loss 2.2914, val loss 2.2922
step 600: train loss 2.2829, val loss 2.2830
step 700: train loss 2.2819, val loss 2.2812
step 800: train loss 2.2789, val loss 2.2794
step 900: train loss 2.2763, val loss 2.2769
step 1000: train loss 2.2763, val loss 2.2767
step 1100: train loss 2.2765, val loss 2.2774
step 1200: train loss 2.2730, val loss 2.2743
step 1300: train loss 2.2688, val loss 2.2705
step 1400: train loss 2.2688, val loss 2.2684
step 1500: train loss 2.2690, val loss 2.2699
step 1600: train loss 2.2678, val loss 2.2655
step 1700: train loss 2.2665, val loss 2.2656
step 1800: train loss 2.2652, val loss 2.2657
step 1900: train loss 2.2597, val loss 2.2614
step 2000: train loss 2.2549, val loss 2.2528
step 2100: train loss 2.2508, val loss 2.2513
step 2200: train loss 2.2431, val loss 2.2455
step 2300: train loss 2.2410, val loss 2.2418
step 2400: train loss 2.2443, val loss 2.2432
step 2500: train loss 2.2400, val loss 2.2391
step 2600: train loss 2.2284, val loss 2.2311
step 2700: train loss 2.2265, val loss 2.2263
step 2800: train loss 2.2302, val loss 2.2292
step 2900: train loss 2.2260, val loss 2.2278
step 3000: train loss 2.2198, val loss 2.2196
step 3100: train loss 2.2172, val loss 2.2258
step 3200: train loss 2.2237, val loss 2.2232
step 3300: train loss 2.2225, val loss 2.2235
step 3400: train loss 2.2170, val loss 2.2177
step 3500: train loss 2.2167, val loss 2.2141
step 3600: train loss 2.2108, val loss 2.2097
step 3700: train loss 2.2100, val loss 2.2096
step 3800: train loss 2.2146, val loss 2.2151
step 3900: train loss 2.2061, val loss 2.2044
step 4000: train loss 2.2142, val loss 2.2126
step 4100: train loss 2.2202, val loss 2.2163
step 4200: train loss 2.2151, val loss 2.2142
step 4300: train loss 2.2048, val loss 2.2043
step 4400: train loss 2.2074, val loss 2.2068
step 4500: train loss 2.2093, val loss 2.2079
step 4600: train loss 2.2026, val loss 2.2027
step 4700: train loss 2.2017, val loss 2.2016
step 4800: train loss 2.2042, val loss 2.2027
step 4900: train loss 2.2120, val loss 2.2094
step 5000: train loss 2.1991, val loss 2.1946
step 5100: train loss 2.1925, val loss 2.1913
step 5200: train loss 2.1925, val loss 2.1959
step 5300: train loss 2.1908, val loss 2.1951
step 5400: train loss 2.1955, val loss 2.1938
step 5500: train loss 2.1967, val loss 2.1968
step 5600: train loss 2.1921, val loss 2.1949
step 5700: train loss 2.1886, val loss 2.1846
step 5800: train loss 2.1939, val loss 2.1952
step 5900: train loss 2.1876, val loss 2.1894
step 6000: train loss 2.1851, val loss 2.1857
step 6100: train loss 2.1900, val loss 2.1906
step 6200: train loss 2.1835, val loss 2.1828
step 6300: train loss 2.1897, val loss 2.1834
step 6400: train loss 2.1824, val loss 2.1821
step 6500: train loss 2.1807, val loss 2.1840
step 6600: train loss 2.1745, val loss 2.1765
step 6700: train loss 2.1732, val loss 2.1728
step 6800: train loss 2.1835, val loss 2.1835
step 6900: train loss 2.1711, val loss 2.1729
step 7000: train loss 2.1780, val loss 2.1792
step 7100: train loss 2.1776, val loss 2.1774
step 7200: train loss 2.1796, val loss 2.1785
step 7300: train loss 2.1702, val loss 2.1719
step 7400: train loss 2.1730, val loss 2.1720
step 7500: train loss 2.1774, val loss 2.1773
step 7600: train loss 2.1655, val loss 2.1687
step 7700: train loss 2.1631, val loss 2.1649
step 7800: train loss 2.1620, val loss 2.1617
step 7900: train loss 2.1645, val loss 2.1674
step 8000: train loss 2.1717, val loss 2.1717
step 8100: train loss 2.1612, val loss 2.1629
step 8200: train loss 2.1705, val loss 2.1691
step 8300: train loss 2.1669, val loss 2.1659
step 8400: train loss 2.1541, val loss 2.1541
step 8500: train loss 2.1564, val loss 2.1500
step 8600: train loss 2.1545, val loss 2.1583
step 8700: train loss 2.1601, val loss 2.1574
step 8800: train loss 2.1527, val loss 2.1570
step 8900: train loss 2.1570, val loss 2.1567
step 9000: train loss 2.1585, val loss 2.1547
step 9100: train loss 2.1563, val loss 2.1592
step 9200: train loss 2.1481, val loss 2.1497
step 9300: train loss 2.1491, val loss 2.1491
step 9400: train loss 2.1475, val loss 2.1510
step 9500: train loss 2.1445, val loss 2.1448
step 9600: train loss 2.1494, val loss 2.1448
step 9700: train loss 2.1485, val loss 2.1455
step 9800: train loss 2.1468, val loss 2.1483
step 9900: train loss 2.1405, val loss 2.1438
step 9999: train loss 2.1477, val loss 2.1499
[2024-09-03 17:57:28,672][HYDRA] 	#1 : learning_rate=3.0000000000000004e-05 dropout=0.0 n_embd=128 batch_size=512
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /home/mila/g/glen.berseth/playground/octo-mini/multirun/2024-09-03/16-32-30/1/wandb/run-20240903_175812-jqt397r1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-monkey-128
wandb: ‚≠êÔ∏è View project at https://wandb.ai/real-lab/mini-grp
wandb: üöÄ View run at https://wandb.ai/real-lab/mini-grp/runs/jqt397r1
wandb: WARNING No relevant files were detected in the specified directory. No code will be logged to your run.
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(data["img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_goal = torch.tensor(data["goal_img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(data["action"][ix], dtype=torch.long)
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.022 MB uploadedwandb: | 0.008 MB of 0.022 MB uploadedwandb: / 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: train loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   val loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: train loss 2.17946
wandb:   val loss 2.17911
wandb: 
wandb: üöÄ View run different-monkey-128 at: https://wandb.ai/real-lab/mini-grp/runs/jqt397r1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/real-lab/mini-grp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240903_175812-jqt397r1/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
cfg: batch_size: 512
block_size: 32
n_patches: 8
max_iters: 10000
eval_interval: 100
learning_rate: 3.0000000000000004e-05
eval_iters: 200
n_embd: 128
r_seed: 1337
n_head: 16
n_blocks: 4
dropout: 0.0
action_bins: 10
image_shape:
- 64
- 64
- 3
dataset: gberseth/mini-bridge-mini64pix
trim: 1000000
error: false

{'batch_size': 512, 'block_size': 32, 'n_patches': 8, 'max_iters': 10000, 'eval_interval': 100, 'learning_rate': 3.0000000000000004e-05, 'eval_iters': 200, 'n_embd': 128, 'r_seed': 1337, 'n_head': 16, 'n_blocks': 4, 'dropout': 0.0, 'action_bins': 10, 'image_shape': [64, 64, 3], 'dataset': 'gberseth/mini-bridge-mini64pix', 'trim': 1000000, 'error': False}
Using device:  cuda (Quadro RTX 8000)
Features: {'img': Image(mode=None, decode=True, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'rotation_delta': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'open_gripper': Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), 'goal': Value(dtype='string', id=None), 'goal_img': Image(mode=None, decode=True, id=None)}
action histogram: [0.11111757 0.11111757 0.11110142 0.11111757 0.13293068 0.08928831
 0.11110142 0.11111757 0.11110142 0.11111757]
bin edges:  [0.  0.9 1.8 2.7 3.6 4.5 5.4 6.3 7.2 8.1 9. ]
Dataset shape: 68766
0.817546 M parameters
step 0: train loss 2.3010, val loss 2.3010
step 100: train loss 2.2991, val loss 2.2990
step 200: train loss 2.2972, val loss 2.2974
step 300: train loss 2.2953, val loss 2.2948
step 400: train loss 2.2937, val loss 2.2938
step 500: train loss 2.2935, val loss 2.2942
step 600: train loss 2.2938, val loss 2.2937
step 700: train loss 2.2912, val loss 2.2920
step 800: train loss 2.2896, val loss 2.2892
step 900: train loss 2.2835, val loss 2.2837
step 1000: train loss 2.2797, val loss 2.2804
step 1100: train loss 2.2790, val loss 2.2792
step 1200: train loss 2.2765, val loss 2.2775
step 1300: train loss 2.2773, val loss 2.2768
step 1400: train loss 2.2743, val loss 2.2745
step 1500: train loss 2.2707, val loss 2.2703
step 1600: train loss 2.2754, val loss 2.2747
step 1700: train loss 2.2689, val loss 2.2695
step 1800: train loss 2.2672, val loss 2.2682
step 1900: train loss 2.2666, val loss 2.2666
step 2000: train loss 2.2649, val loss 2.2634
step 2100: train loss 2.2628, val loss 2.2625
step 2200: train loss 2.2610, val loss 2.2650
step 2300: train loss 2.2637, val loss 2.2621
step 2400: train loss 2.2592, val loss 2.2590
step 2500: train loss 2.2591, val loss 2.2598
step 2600: train loss 2.2568, val loss 2.2558
step 2700: train loss 2.2564, val loss 2.2567
step 2800: train loss 2.2517, val loss 2.2522
step 2900: train loss 2.2531, val loss 2.2517
step 3000: train loss 2.2492, val loss 2.2491
step 3100: train loss 2.2449, val loss 2.2474
step 3200: train loss 2.2461, val loss 2.2460
step 3300: train loss 2.2447, val loss 2.2462
step 3400: train loss 2.2442, val loss 2.2444
step 3500: train loss 2.2432, val loss 2.2423
step 3600: train loss 2.2398, val loss 2.2406
step 3700: train loss 2.2525, val loss 2.2520
step 3800: train loss 2.2412, val loss 2.2428
step 3900: train loss 2.2416, val loss 2.2425
step 4000: train loss 2.2374, val loss 2.2386
step 4100: train loss 2.2382, val loss 2.2369
step 4200: train loss 2.2371, val loss 2.2354
step 4300: train loss 2.2349, val loss 2.2339
step 4400: train loss 2.2362, val loss 2.2366
step 4500: train loss 2.2302, val loss 2.2297
step 4600: train loss 2.2312, val loss 2.2326
step 4700: train loss 2.2292, val loss 2.2294
step 4800: train loss 2.2263, val loss 2.2268
step 4900: train loss 2.2268, val loss 2.2251
step 5000: train loss 2.2290, val loss 2.2281
step 5100: train loss 2.2232, val loss 2.2233
step 5200: train loss 2.2240, val loss 2.2251
step 5300: train loss 2.2200, val loss 2.2232
step 5400: train loss 2.2206, val loss 2.2214
step 5500: train loss 2.2191, val loss 2.2180
step 5600: train loss 2.2202, val loss 2.2200
step 5700: train loss 2.2164, val loss 2.2174
step 5800: train loss 2.2168, val loss 2.2172
step 5900: train loss 2.2146, val loss 2.2174
step 6000: train loss 2.2230, val loss 2.2231
step 6100: train loss 2.2213, val loss 2.2204
step 6200: train loss 2.2105, val loss 2.2100
step 6300: train loss 2.2079, val loss 2.2110
step 6400: train loss 2.2158, val loss 2.2139
step 6500: train loss 2.2081, val loss 2.2103
step 6600: train loss 2.2049, val loss 2.2074
step 6700: train loss 2.2076, val loss 2.2079
step 6800: train loss 2.2054, val loss 2.2034
step 6900: train loss 2.2044, val loss 2.2029
step 7000: train loss 2.2036, val loss 2.2027
step 7100: train loss 2.1996, val loss 2.2004
step 7200: train loss 2.2083, val loss 2.2056
step 7300: train loss 2.2030, val loss 2.2039
step 7400: train loss 2.2000, val loss 2.1984
step 7500: train loss 2.2040, val loss 2.2035
step 7600: train loss 2.1975, val loss 2.1971
step 7700: train loss 2.1975, val loss 2.1979
step 7800: train loss 2.1914, val loss 2.1922
step 7900: train loss 2.1936, val loss 2.1929
step 8000: train loss 2.1944, val loss 2.1937
step 8100: train loss 2.2075, val loss 2.2098
step 8200: train loss 2.1862, val loss 2.1883
step 8300: train loss 2.1977, val loss 2.1951
step 8400: train loss 2.1983, val loss 2.1966
step 8500: train loss 2.1897, val loss 2.1909
step 8600: train loss 2.1857, val loss 2.1862
step 8700: train loss 2.1946, val loss 2.1924
step 8800: train loss 2.1936, val loss 2.1947
step 8900: train loss 2.1810, val loss 2.1811
step 9000: train loss 2.1931, val loss 2.1929
step 9100: train loss 2.1830, val loss 2.1849
step 9200: train loss 2.1871, val loss 2.1873
step 9300: train loss 2.1784, val loss 2.1773
step 9400: train loss 2.1834, val loss 2.1825
step 9500: train loss 2.1899, val loss 2.1879
step 9600: train loss 2.1764, val loss 2.1759
step 9700: train loss 2.1762, val loss 2.1754
step 9800: train loss 2.1837, val loss 2.1791
step 9900: train loss 2.1777, val loss 2.1765
step 9999: train loss 2.1795, val loss 2.1791
[2024-09-03 19:35:16,669][HYDRA] Launching 2 jobs locally
[2024-09-03 19:35:16,669][HYDRA] 	#2 : learning_rate=7.000000000000001e-05 dropout=0.0 n_embd=64 batch_size=512
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /home/mila/g/glen.berseth/playground/octo-mini/multirun/2024-09-03/16-32-30/2/wandb/run-20240903_193600-d1fj48z2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-butterfly-129
wandb: ‚≠êÔ∏è View project at https://wandb.ai/real-lab/mini-grp
wandb: üöÄ View run at https://wandb.ai/real-lab/mini-grp/runs/d1fj48z2
wandb: WARNING No relevant files were detected in the specified directory. No code will be logged to your run.
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(data["img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_goal = torch.tensor(data["goal_img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(data["action"][ix], dtype=torch.long)
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.008 MB uploadedwandb: | 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: train loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:   val loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: train loss 2.19139
wandb:   val loss 2.18959
wandb: 
wandb: üöÄ View run swift-butterfly-129 at: https://wandb.ai/real-lab/mini-grp/runs/d1fj48z2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/real-lab/mini-grp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240903_193600-d1fj48z2/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
cfg: batch_size: 512
block_size: 32
n_patches: 8
max_iters: 10000
eval_interval: 100
learning_rate: 7.000000000000001e-05
eval_iters: 200
n_embd: 64
r_seed: 1337
n_head: 16
n_blocks: 4
dropout: 0.0
action_bins: 10
image_shape:
- 64
- 64
- 3
dataset: gberseth/mini-bridge-mini64pix
trim: 1000000
error: false

{'batch_size': 512, 'block_size': 32, 'n_patches': 8, 'max_iters': 10000, 'eval_interval': 100, 'learning_rate': 7.000000000000001e-05, 'eval_iters': 200, 'n_embd': 64, 'r_seed': 1337, 'n_head': 16, 'n_blocks': 4, 'dropout': 0.0, 'action_bins': 10, 'image_shape': [64, 64, 3], 'dataset': 'gberseth/mini-bridge-mini64pix', 'trim': 1000000, 'error': False}
Using device:  cuda (Quadro RTX 8000)
Features: {'img': Image(mode=None, decode=True, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'rotation_delta': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'open_gripper': Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), 'goal': Value(dtype='string', id=None), 'goal_img': Image(mode=None, decode=True, id=None)}
action histogram: [0.11111757 0.11111757 0.11110142 0.11111757 0.13293068 0.08928831
 0.11110142 0.11111757 0.11110142 0.11111757]
bin edges:  [0.  0.9 1.8 2.7 3.6 4.5 5.4 6.3 7.2 8.1 9. ]
Dataset shape: 68766
0.21217 M parameters
step 0: train loss 2.3032, val loss 2.3037
step 100: train loss 2.2983, val loss 2.2986
step 200: train loss 2.2954, val loss 2.2954
step 300: train loss 2.2946, val loss 2.2947
step 400: train loss 2.2931, val loss 2.2929
step 500: train loss 2.2918, val loss 2.2929
step 600: train loss 2.2918, val loss 2.2919
step 700: train loss 2.2906, val loss 2.2905
step 800: train loss 2.2880, val loss 2.2894
step 900: train loss 2.2865, val loss 2.2862
step 1000: train loss 2.2853, val loss 2.2854
step 1100: train loss 2.2810, val loss 2.2801
step 1200: train loss 2.2780, val loss 2.2780
step 1300: train loss 2.2745, val loss 2.2746
step 1400: train loss 2.2730, val loss 2.2732
step 1500: train loss 2.2723, val loss 2.2730
step 1600: train loss 2.2717, val loss 2.2703
step 1700: train loss 2.2696, val loss 2.2691
step 1800: train loss 2.2663, val loss 2.2669
step 1900: train loss 2.2659, val loss 2.2650
step 2000: train loss 2.2652, val loss 2.2648
step 2100: train loss 2.2624, val loss 2.2614
step 2200: train loss 2.2634, val loss 2.2631
step 2300: train loss 2.2617, val loss 2.2610
step 2400: train loss 2.2478, val loss 2.2481
step 2500: train loss 2.2499, val loss 2.2465
step 2600: train loss 2.2440, val loss 2.2433
step 2700: train loss 2.2470, val loss 2.2498
step 2800: train loss 2.2369, val loss 2.2394
step 2900: train loss 2.2346, val loss 2.2338
step 3000: train loss 2.2362, val loss 2.2352
step 3100: train loss 2.2366, val loss 2.2371
step 3200: train loss 2.2318, val loss 2.2334
step 3300: train loss 2.2326, val loss 2.2302
step 3400: train loss 2.2301, val loss 2.2309
step 3500: train loss 2.2296, val loss 2.2299
step 3600: train loss 2.2301, val loss 2.2291
step 3700: train loss 2.2259, val loss 2.2271
step 3800: train loss 2.2226, val loss 2.2228
step 3900: train loss 2.2248, val loss 2.2243
step 4000: train loss 2.2234, val loss 2.2220
step 4100: train loss 2.2223, val loss 2.2232
step 4200: train loss 2.2213, val loss 2.2204
step 4300: train loss 2.2184, val loss 2.2182
step 4400: train loss 2.2181, val loss 2.2189
step 4500: train loss 2.2173, val loss 2.2185
step 4600: train loss 2.2219, val loss 2.2236
step 4700: train loss 2.2156, val loss 2.2150
step 4800: train loss 2.2174, val loss 2.2151
step 4900: train loss 2.2128, val loss 2.2150
step 5000: train loss 2.2140, val loss 2.2165
step 5100: train loss 2.2113, val loss 2.2122
step 5200: train loss 2.2153, val loss 2.2165
step 5300: train loss 2.2105, val loss 2.2101
step 5400: train loss 2.2157, val loss 2.2142
step 5500: train loss 2.2136, val loss 2.2146
step 5600: train loss 2.2106, val loss 2.2128
step 5700: train loss 2.2072, val loss 2.2070
step 5800: train loss 2.2102, val loss 2.2105
step 5900: train loss 2.2097, val loss 2.2115
step 6000: train loss 2.2106, val loss 2.2116
step 6100: train loss 2.2099, val loss 2.2126
step 6200: train loss 2.2071, val loss 2.2070
step 6300: train loss 2.2037, val loss 2.2044
step 6400: train loss 2.2099, val loss 2.2092
step 6500: train loss 2.2048, val loss 2.2058
step 6600: train loss 2.2033, val loss 2.2040
step 6700: train loss 2.2051, val loss 2.2042
step 6800: train loss 2.2019, val loss 2.1994
step 6900: train loss 2.2025, val loss 2.2013
step 7000: train loss 2.2035, val loss 2.2044
step 7100: train loss 2.2086, val loss 2.2071
step 7200: train loss 2.2033, val loss 2.2011
step 7300: train loss 2.2002, val loss 2.2010
step 7400: train loss 2.2043, val loss 2.2044
step 7500: train loss 2.2024, val loss 2.2003
step 7600: train loss 2.2120, val loss 2.2127
step 7700: train loss 2.1992, val loss 2.1997
step 7800: train loss 2.1960, val loss 2.1960
step 7900: train loss 2.2003, val loss 2.1997
step 8000: train loss 2.1966, val loss 2.1961
step 8100: train loss 2.1949, val loss 2.1956
step 8200: train loss 2.2091, val loss 2.2083
step 8300: train loss 2.1934, val loss 2.1939
step 8400: train loss 2.1954, val loss 2.1939
step 8500: train loss 2.1968, val loss 2.1977
step 8600: train loss 2.1939, val loss 2.1911
step 8700: train loss 2.1937, val loss 2.1926
step 8800: train loss 2.1946, val loss 2.1953
step 8900: train loss 2.1916, val loss 2.1932
step 9000: train loss 2.1891, val loss 2.1891
step 9100: train loss 2.1936, val loss 2.1933
step 9200: train loss 2.1948, val loss 2.1939
step 9300: train loss 2.1912, val loss 2.1883
step 9400: train loss 2.2027, val loss 2.2018
step 9500: train loss 2.1915, val loss 2.1935
step 9600: train loss 2.1888, val loss 2.1916
step 9700: train loss 2.1848, val loss 2.1869
step 9800: train loss 2.1905, val loss 2.1899
step 9900: train loss 2.1859, val loss 2.1868
step 9999: train loss 2.1914, val loss 2.1896
[2024-09-03 20:51:20,587][HYDRA] 	#3 : learning_rate=1e-05 dropout=0.2 n_embd=512 batch_size=256
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /home/mila/g/glen.berseth/playground/octo-mini/multirun/2024-09-03/16-32-30/3/wandb/run-20240903_205204-p93ecxs5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-dust-130
wandb: ‚≠êÔ∏è View project at https://wandb.ai/real-lab/mini-grp
wandb: üöÄ View run at https://wandb.ai/real-lab/mini-grp/runs/p93ecxs5
wandb: WARNING No relevant files were detected in the specified directory. No code will be logged to your run.
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(data["img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_goal = torch.tensor(data["goal_img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(data["action"][ix], dtype=torch.long)
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.022 MB uploadedwandb: | 0.008 MB of 0.022 MB uploadedwandb: / 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: train loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:   val loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: train loss 2.19842
wandb:   val loss 2.19716
wandb: 
wandb: üöÄ View run azure-dust-130 at: https://wandb.ai/real-lab/mini-grp/runs/p93ecxs5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/real-lab/mini-grp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240903_205204-p93ecxs5/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
cfg: batch_size: 256
block_size: 32
n_patches: 8
max_iters: 10000
eval_interval: 100
learning_rate: 1.0e-05
eval_iters: 200
n_embd: 512
r_seed: 1337
n_head: 16
n_blocks: 4
dropout: 0.2
action_bins: 10
image_shape:
- 64
- 64
- 3
dataset: gberseth/mini-bridge-mini64pix
trim: 1000000
error: false

{'batch_size': 256, 'block_size': 32, 'n_patches': 8, 'max_iters': 10000, 'eval_interval': 100, 'learning_rate': 1e-05, 'eval_iters': 200, 'n_embd': 512, 'r_seed': 1337, 'n_head': 16, 'n_blocks': 4, 'dropout': 0.2, 'action_bins': 10, 'image_shape': [64, 64, 3], 'dataset': 'gberseth/mini-bridge-mini64pix', 'trim': 1000000, 'error': False}
Using device:  cuda (Quadro RTX 8000)
Features: {'img': Image(mode=None, decode=True, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'rotation_delta': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'open_gripper': Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), 'goal': Value(dtype='string', id=None), 'goal_img': Image(mode=None, decode=True, id=None)}
action histogram: [0.11111757 0.11111757 0.11110142 0.11111757 0.13293068 0.08928831
 0.11110142 0.11111757 0.11110142 0.11111757]
bin edges:  [0.  0.9 1.8 2.7 3.6 4.5 5.4 6.3 7.2 8.1 9. ]
Dataset shape: 68766
12.707338 M parameters
step 0: train loss 2.3041, val loss 2.3040
step 100: train loss 2.2991, val loss 2.2983
step 200: train loss 2.2968, val loss 2.2975
step 300: train loss 2.2956, val loss 2.2957
step 400: train loss 2.2947, val loss 2.2944
step 500: train loss 2.2951, val loss 2.2960
step 600: train loss 2.2954, val loss 2.2947
step 700: train loss 2.2928, val loss 2.2936
step 800: train loss 2.2962, val loss 2.2953
step 900: train loss 2.2913, val loss 2.2923
step 1000: train loss 2.2913, val loss 2.2909
step 1100: train loss 2.2901, val loss 2.2885
step 1200: train loss 2.2884, val loss 2.2889
step 1300: train loss 2.2821, val loss 2.2812
step 1400: train loss 2.2807, val loss 2.2820
step 1500: train loss 2.2791, val loss 2.2788
step 1600: train loss 2.2778, val loss 2.2772
step 1700: train loss 2.2795, val loss 2.2796
step 1800: train loss 2.2780, val loss 2.2784
step 1900: train loss 2.2746, val loss 2.2743
step 2000: train loss 2.2746, val loss 2.2761
step 2100: train loss 2.2728, val loss 2.2741
step 2200: train loss 2.2723, val loss 2.2726
step 2300: train loss 2.2772, val loss 2.2768
step 2400: train loss 2.2703, val loss 2.2703
step 2500: train loss 2.2719, val loss 2.2707
step 2600: train loss 2.2733, val loss 2.2740
step 2700: train loss 2.2681, val loss 2.2691
step 2800: train loss 2.2689, val loss 2.2702
step 2900: train loss 2.2689, val loss 2.2678
step 3000: train loss 2.2685, val loss 2.2698
step 3100: train loss 2.2619, val loss 2.2632
step 3200: train loss 2.2593, val loss 2.2591
step 3300: train loss 2.2540, val loss 2.2552
step 3400: train loss 2.2527, val loss 2.2517
step 3500: train loss 2.2462, val loss 2.2490
step 3600: train loss 2.2401, val loss 2.2407
step 3700: train loss 2.2423, val loss 2.2429
step 3800: train loss 2.2392, val loss 2.2379
step 3900: train loss 2.2375, val loss 2.2371
step 4000: train loss 2.2373, val loss 2.2348
step 4100: train loss 2.2344, val loss 2.2330
step 4200: train loss 2.2349, val loss 2.2373
step 4300: train loss 2.2345, val loss 2.2346
step 4400: train loss 2.2320, val loss 2.2312
step 4500: train loss 2.2355, val loss 2.2347
step 4600: train loss 2.2327, val loss 2.2334
step 4700: train loss 2.2286, val loss 2.2308
step 4800: train loss 2.2264, val loss 2.2268
step 4900: train loss 2.2282, val loss 2.2293
step 5000: train loss 2.2273, val loss 2.2254
step 5100: train loss 2.2259, val loss 2.2273
step 5200: train loss 2.2251, val loss 2.2259
step 5300: train loss 2.2254, val loss 2.2258
step 5400: train loss 2.2239, val loss 2.2288
step 5500: train loss 2.2216, val loss 2.2225
step 5600: train loss 2.2290, val loss 2.2296
step 5700: train loss 2.2226, val loss 2.2259
step 5800: train loss 2.2284, val loss 2.2250
step 5900: train loss 2.2201, val loss 2.2186
step 6000: train loss 2.2226, val loss 2.2185
step 6100: train loss 2.2223, val loss 2.2224
step 6200: train loss 2.2260, val loss 2.2260
step 6300: train loss 2.2211, val loss 2.2172
step 6400: train loss 2.2191, val loss 2.2197
step 6500: train loss 2.2174, val loss 2.2187
step 6600: train loss 2.2129, val loss 2.2147
step 6700: train loss 2.2187, val loss 2.2130
step 6800: train loss 2.2144, val loss 2.2158
step 6900: train loss 2.2161, val loss 2.2116
step 7000: train loss 2.2182, val loss 2.2154
step 7100: train loss 2.2099, val loss 2.2122
step 7200: train loss 2.2100, val loss 2.2117
step 7300: train loss 2.2121, val loss 2.2164
step 7400: train loss 2.2120, val loss 2.2111
step 7500: train loss 2.2102, val loss 2.2087
step 7600: train loss 2.2113, val loss 2.2059
step 7700: train loss 2.2089, val loss 2.2105
step 7800: train loss 2.2131, val loss 2.2135
step 7900: train loss 2.2104, val loss 2.2115
step 8000: train loss 2.2104, val loss 2.2098
step 8100: train loss 2.2087, val loss 2.2083
step 8200: train loss 2.2110, val loss 2.2113
step 8300: train loss 2.2046, val loss 2.2059
step 8400: train loss 2.2029, val loss 2.2033
step 8500: train loss 2.2050, val loss 2.2026
step 8600: train loss 2.2068, val loss 2.2065
step 8700: train loss 2.2038, val loss 2.2048
step 8800: train loss 2.2071, val loss 2.2071
step 8900: train loss 2.2056, val loss 2.2032
step 9000: train loss 2.2031, val loss 2.2021
step 9100: train loss 2.2058, val loss 2.2042
step 9200: train loss 2.2007, val loss 2.2022
step 9300: train loss 2.2003, val loss 2.2025
step 9400: train loss 2.2060, val loss 2.2046
step 9500: train loss 2.2036, val loss 2.2017
step 9600: train loss 2.2008, val loss 2.1994
step 9700: train loss 2.1973, val loss 2.2013
step 9800: train loss 2.2023, val loss 2.2003
step 9900: train loss 2.1984, val loss 2.1984
step 9999: train loss 2.1984, val loss 2.1972
[2024-09-03 23:39:15,671][HYDRA] Launching 2 jobs locally
[2024-09-03 23:39:15,671][HYDRA] 	#4 : learning_rate=7.000000000000001e-05 dropout=0.2 n_embd=512 batch_size=512
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /home/mila/g/glen.berseth/playground/octo-mini/multirun/2024-09-03/16-32-30/4/wandb/run-20240903_233958-7ut0zijh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-planet-131
wandb: ‚≠êÔ∏è View project at https://wandb.ai/real-lab/mini-grp
wandb: üöÄ View run at https://wandb.ai/real-lab/mini-grp/runs/7ut0zijh
wandb: WARNING No relevant files were detected in the specified directory. No code will be logged to your run.
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(data["img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_goal = torch.tensor(data["goal_img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(data["action"][ix], dtype=torch.long)
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.023 MB uploadedwandb: | 0.008 MB of 0.023 MB uploadedwandb: / 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: train loss ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val loss ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: train loss 2.01206
wandb:   val loss 2.01409
wandb: 
wandb: üöÄ View run fine-planet-131 at: https://wandb.ai/real-lab/mini-grp/runs/7ut0zijh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/real-lab/mini-grp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240903_233958-7ut0zijh/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
cfg: batch_size: 512
block_size: 32
n_patches: 8
max_iters: 10000
eval_interval: 100
learning_rate: 7.000000000000001e-05
eval_iters: 200
n_embd: 512
r_seed: 1337
n_head: 16
n_blocks: 4
dropout: 0.2
action_bins: 10
image_shape:
- 64
- 64
- 3
dataset: gberseth/mini-bridge-mini64pix
trim: 1000000
error: false

{'batch_size': 512, 'block_size': 32, 'n_patches': 8, 'max_iters': 10000, 'eval_interval': 100, 'learning_rate': 7.000000000000001e-05, 'eval_iters': 200, 'n_embd': 512, 'r_seed': 1337, 'n_head': 16, 'n_blocks': 4, 'dropout': 0.2, 'action_bins': 10, 'image_shape': [64, 64, 3], 'dataset': 'gberseth/mini-bridge-mini64pix', 'trim': 1000000, 'error': False}
Using device:  cuda (Quadro RTX 8000)
Features: {'img': Image(mode=None, decode=True, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'rotation_delta': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'open_gripper': Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), 'goal': Value(dtype='string', id=None), 'goal_img': Image(mode=None, decode=True, id=None)}
action histogram: [0.11111757 0.11111757 0.11110142 0.11111757 0.13293068 0.08928831
 0.11110142 0.11111757 0.11110142 0.11111757]
bin edges:  [0.  0.9 1.8 2.7 3.6 4.5 5.4 6.3 7.2 8.1 9. ]
Dataset shape: 68766
12.707338 M parameters
step 0: train loss 2.3044, val loss 2.3045
step 100: train loss 2.3047, val loss 2.3047
step 200: train loss 2.2966, val loss 2.2963
step 300: train loss 2.2921, val loss 2.2893
step 400: train loss 2.2858, val loss 2.2851
step 500: train loss 2.2792, val loss 2.2788
step 600: train loss 2.2729, val loss 2.2730
step 700: train loss 2.2687, val loss 2.2684
step 800: train loss 2.2645, val loss 2.2628
step 900: train loss 2.2478, val loss 2.2477
step 1000: train loss 2.2374, val loss 2.2380
step 1100: train loss 2.2353, val loss 2.2342
step 1200: train loss 2.2210, val loss 2.2200
step 1300: train loss 2.2222, val loss 2.2229
step 1400: train loss 2.2188, val loss 2.2181
step 1500: train loss 2.2176, val loss 2.2186
step 1600: train loss 2.2038, val loss 2.2084
step 1700: train loss 2.2045, val loss 2.2056
step 1800: train loss 2.1992, val loss 2.2017
step 1900: train loss 2.2030, val loss 2.2033
step 2000: train loss 2.1961, val loss 2.1950
step 2100: train loss 2.1916, val loss 2.1949
step 2200: train loss 2.1960, val loss 2.1948
step 2300: train loss 2.1886, val loss 2.1894
step 2400: train loss 2.1885, val loss 2.1882
step 2500: train loss 2.1877, val loss 2.1871
step 2600: train loss 2.1800, val loss 2.1775
step 2700: train loss 2.1807, val loss 2.1780
step 2800: train loss 2.1738, val loss 2.1756
step 2900: train loss 2.1700, val loss 2.1714
step 3000: train loss 2.1727, val loss 2.1731
step 3100: train loss 2.1673, val loss 2.1699
step 3200: train loss 2.1642, val loss 2.1643
step 3300: train loss 2.1641, val loss 2.1641
step 3400: train loss 2.1589, val loss 2.1597
step 3500: train loss 2.1547, val loss 2.1565
step 3600: train loss 2.1604, val loss 2.1581
step 3700: train loss 2.1498, val loss 2.1477
step 3800: train loss 2.1448, val loss 2.1419
step 3900: train loss 2.1523, val loss 2.1532
step 4000: train loss 2.1452, val loss 2.1477
step 4100: train loss 2.1388, val loss 2.1433
step 4200: train loss 2.1351, val loss 2.1337
step 4300: train loss 2.1418, val loss 2.1416
step 4400: train loss 2.1290, val loss 2.1295
step 4500: train loss 2.1278, val loss 2.1250
step 4600: train loss 2.1333, val loss 2.1360
step 4700: train loss 2.1227, val loss 2.1221
step 4800: train loss 2.1219, val loss 2.1265
step 4900: train loss 2.1133, val loss 2.1145
step 5000: train loss 2.1171, val loss 2.1178
step 5100: train loss 2.1153, val loss 2.1148
step 5200: train loss 2.1112, val loss 2.1134
step 5300: train loss 2.1075, val loss 2.1094
step 5400: train loss 2.1093, val loss 2.1084
step 5500: train loss 2.1056, val loss 2.1054
step 5600: train loss 2.1090, val loss 2.1088
step 5700: train loss 2.1020, val loss 2.1056
step 5800: train loss 2.1039, val loss 2.1028
step 5900: train loss 2.1006, val loss 2.0995
step 6000: train loss 2.0942, val loss 2.0964
step 6100: train loss 2.1002, val loss 2.1017
step 6200: train loss 2.0842, val loss 2.0826
step 6300: train loss 2.0821, val loss 2.0818
step 6400: train loss 2.0915, val loss 2.0938
step 6500: train loss 2.0767, val loss 2.0765
step 6600: train loss 2.0728, val loss 2.0741
step 6700: train loss 2.0800, val loss 2.0780
step 6800: train loss 2.0854, val loss 2.0849
step 6900: train loss 2.0770, val loss 2.0760
step 7000: train loss 2.0741, val loss 2.0698
step 7100: train loss 2.0621, val loss 2.0637
step 7200: train loss 2.0741, val loss 2.0746
step 7300: train loss 2.0642, val loss 2.0676
step 7400: train loss 2.0693, val loss 2.0688
step 7500: train loss 2.0583, val loss 2.0606
step 7600: train loss 2.0663, val loss 2.0671
step 7700: train loss 2.0548, val loss 2.0540
step 7800: train loss 2.0523, val loss 2.0527
step 7900: train loss 2.0604, val loss 2.0615
step 8000: train loss 2.0540, val loss 2.0550
step 8100: train loss 2.0537, val loss 2.0515
step 8200: train loss 2.0487, val loss 2.0464
step 8300: train loss 2.0414, val loss 2.0422
step 8400: train loss 2.0422, val loss 2.0386
step 8500: train loss 2.0457, val loss 2.0439
step 8600: train loss 2.0397, val loss 2.0393
step 8700: train loss 2.0309, val loss 2.0278
step 8800: train loss 2.0331, val loss 2.0325
step 8900: train loss 2.0323, val loss 2.0299
step 9000: train loss 2.0313, val loss 2.0325
step 9100: train loss 2.0214, val loss 2.0168
step 9200: train loss 2.0244, val loss 2.0252
step 9300: train loss 2.0216, val loss 2.0235
step 9400: train loss 2.0190, val loss 2.0178
step 9500: train loss 2.0215, val loss 2.0205
step 9600: train loss 2.0184, val loss 2.0152
step 9700: train loss 2.0208, val loss 2.0203
step 9800: train loss 2.0261, val loss 2.0264
step 9900: train loss 2.0164, val loss 2.0181
step 9999: train loss 2.0121, val loss 2.0141
[2024-09-04 05:12:02,481][HYDRA] 	#5 : learning_rate=1e-05 dropout=0.0 n_embd=512 batch_size=256
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /home/mila/g/glen.berseth/playground/octo-mini/multirun/2024-09-03/16-32-30/5/wandb/run-20240904_051245-cp371apr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-eon-132
wandb: ‚≠êÔ∏è View project at https://wandb.ai/real-lab/mini-grp
wandb: üöÄ View run at https://wandb.ai/real-lab/mini-grp/runs/cp371apr
wandb: WARNING No relevant files were detected in the specified directory. No code will be logged to your run.
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(data["img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_goal = torch.tensor(data["goal_img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(data["action"][ix], dtype=torch.long)
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.008 MB uploadedwandb: | 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: train loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val loss ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: train loss 2.14908
wandb:   val loss 2.1538
wandb: 
wandb: üöÄ View run rosy-eon-132 at: https://wandb.ai/real-lab/mini-grp/runs/cp371apr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/real-lab/mini-grp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240904_051245-cp371apr/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
cfg: batch_size: 256
block_size: 32
n_patches: 8
max_iters: 10000
eval_interval: 100
learning_rate: 1.0e-05
eval_iters: 200
n_embd: 512
r_seed: 1337
n_head: 16
n_blocks: 4
dropout: 0.0
action_bins: 10
image_shape:
- 64
- 64
- 3
dataset: gberseth/mini-bridge-mini64pix
trim: 1000000
error: false

{'batch_size': 256, 'block_size': 32, 'n_patches': 8, 'max_iters': 10000, 'eval_interval': 100, 'learning_rate': 1e-05, 'eval_iters': 200, 'n_embd': 512, 'r_seed': 1337, 'n_head': 16, 'n_blocks': 4, 'dropout': 0.0, 'action_bins': 10, 'image_shape': [64, 64, 3], 'dataset': 'gberseth/mini-bridge-mini64pix', 'trim': 1000000, 'error': False}
Using device:  cuda (Quadro RTX 8000)
Features: {'img': Image(mode=None, decode=True, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'rotation_delta': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'open_gripper': Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), 'goal': Value(dtype='string', id=None), 'goal_img': Image(mode=None, decode=True, id=None)}
action histogram: [0.11111757 0.11111757 0.11110142 0.11111757 0.13293068 0.08928831
 0.11110142 0.11111757 0.11110142 0.11111757]
bin edges:  [0.  0.9 1.8 2.7 3.6 4.5 5.4 6.3 7.2 8.1 9. ]
Dataset shape: 68766
12.707338 M parameters
step 0: train loss 2.3048, val loss 2.3038
step 100: train loss 2.2996, val loss 2.2994
step 200: train loss 2.2965, val loss 2.2969
step 300: train loss 2.2959, val loss 2.2963
step 400: train loss 2.2944, val loss 2.2940
step 500: train loss 2.2945, val loss 2.2951
step 600: train loss 2.2934, val loss 2.2944
step 700: train loss 2.2924, val loss 2.2933
step 800: train loss 2.2890, val loss 2.2889
step 900: train loss 2.2843, val loss 2.2846
step 1000: train loss 2.2833, val loss 2.2819
step 1100: train loss 2.2793, val loss 2.2790
step 1200: train loss 2.2761, val loss 2.2762
step 1300: train loss 2.2753, val loss 2.2763
step 1400: train loss 2.2731, val loss 2.2734
step 1500: train loss 2.2752, val loss 2.2750
step 1600: train loss 2.2720, val loss 2.2725
step 1700: train loss 2.2711, val loss 2.2700
step 1800: train loss 2.2678, val loss 2.2676
step 1900: train loss 2.2649, val loss 2.2666
step 2000: train loss 2.2666, val loss 2.2669
step 2100: train loss 2.2632, val loss 2.2642
step 2200: train loss 2.2604, val loss 2.2626
step 2300: train loss 2.2596, val loss 2.2570
step 2400: train loss 2.2545, val loss 2.2519
step 2500: train loss 2.2519, val loss 2.2489
step 2600: train loss 2.2421, val loss 2.2427
step 2700: train loss 2.2371, val loss 2.2397
step 2800: train loss 2.2401, val loss 2.2421
step 2900: train loss 2.2383, val loss 2.2408
step 3000: train loss 2.2301, val loss 2.2287
step 3100: train loss 2.2275, val loss 2.2243
step 3200: train loss 2.2256, val loss 2.2237
step 3300: train loss 2.2239, val loss 2.2219
step 3400: train loss 2.2196, val loss 2.2188
step 3500: train loss 2.2226, val loss 2.2202
step 3600: train loss 2.2152, val loss 2.2152
step 3700: train loss 2.2155, val loss 2.2171
step 3800: train loss 2.2160, val loss 2.2139
step 3900: train loss 2.2185, val loss 2.2180
step 4000: train loss 2.2139, val loss 2.2145
step 4100: train loss 2.2117, val loss 2.2124
step 4200: train loss 2.2066, val loss 2.2096
step 4300: train loss 2.2119, val loss 2.2118
step 4400: train loss 2.2129, val loss 2.2138
step 4500: train loss 2.2093, val loss 2.2105
step 4600: train loss 2.2044, val loss 2.2067
step 4700: train loss 2.2018, val loss 2.2037
step 4800: train loss 2.2040, val loss 2.2055
step 4900: train loss 2.2064, val loss 2.2063
step 5000: train loss 2.2064, val loss 2.2083
step 5100: train loss 2.2062, val loss 2.2057
step 5200: train loss 2.2008, val loss 2.2048
step 5300: train loss 2.1998, val loss 2.1999
step 5400: train loss 2.2055, val loss 2.2035
step 5500: train loss 2.1993, val loss 2.1950
step 5600: train loss 2.1953, val loss 2.1972
step 5700: train loss 2.1953, val loss 2.1982
step 5800: train loss 2.1967, val loss 2.1952
step 5900: train loss 2.1949, val loss 2.1908
step 6000: train loss 2.1962, val loss 2.1927
step 6100: train loss 2.1891, val loss 2.1893
step 6200: train loss 2.1925, val loss 2.1904
step 6300: train loss 2.1891, val loss 2.1880
step 6400: train loss 2.1882, val loss 2.1857
step 6500: train loss 2.1869, val loss 2.1877
step 6600: train loss 2.1890, val loss 2.1873
step 6700: train loss 2.1910, val loss 2.1899
step 6800: train loss 2.1822, val loss 2.1836
step 6900: train loss 2.1820, val loss 2.1808
step 7000: train loss 2.1843, val loss 2.1882
step 7100: train loss 2.1828, val loss 2.1799
step 7200: train loss 2.1815, val loss 2.1809
step 7300: train loss 2.1796, val loss 2.1781
step 7400: train loss 2.1809, val loss 2.1819
step 7500: train loss 2.1755, val loss 2.1760
step 7600: train loss 2.1777, val loss 2.1818
step 7700: train loss 2.1748, val loss 2.1747
step 7800: train loss 2.1774, val loss 2.1780
step 7900: train loss 2.1713, val loss 2.1717
step 8000: train loss 2.1701, val loss 2.1693
step 8100: train loss 2.1682, val loss 2.1704
step 8200: train loss 2.1735, val loss 2.1699
step 8300: train loss 2.1722, val loss 2.1698
step 8400: train loss 2.1631, val loss 2.1615
step 8500: train loss 2.1679, val loss 2.1661
step 8600: train loss 2.1605, val loss 2.1559
step 8700: train loss 2.1568, val loss 2.1614
step 8800: train loss 2.1568, val loss 2.1564
step 8900: train loss 2.1577, val loss 2.1594
step 9000: train loss 2.1561, val loss 2.1575
step 9100: train loss 2.1541, val loss 2.1559
step 9200: train loss 2.1538, val loss 2.1552
step 9300: train loss 2.1554, val loss 2.1564
step 9400: train loss 2.1561, val loss 2.1514
step 9500: train loss 2.1465, val loss 2.1460
step 9600: train loss 2.1543, val loss 2.1540
step 9700: train loss 2.1515, val loss 2.1536
step 9800: train loss 2.1508, val loss 2.1487
step 9900: train loss 2.1440, val loss 2.1457
step 9999: train loss 2.1491, val loss 2.1538
[2024-09-04 07:58:08,413][HYDRA] Launching 2 jobs locally
[2024-09-04 07:58:08,414][HYDRA] 	#6 : learning_rate=7.000000000000001e-05 dropout=0.2 n_embd=64 batch_size=512
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /home/mila/g/glen.berseth/playground/octo-mini/multirun/2024-09-03/16-32-30/6/wandb/run-20240904_075851-b80imqu9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-glade-133
wandb: ‚≠êÔ∏è View project at https://wandb.ai/real-lab/mini-grp
wandb: üöÄ View run at https://wandb.ai/real-lab/mini-grp/runs/b80imqu9
wandb: WARNING No relevant files were detected in the specified directory. No code will be logged to your run.
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(data["img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_goal = torch.tensor(data["goal_img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(data["action"][ix], dtype=torch.long)
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.008 MB uploadedwandb: | 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: train loss ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:   val loss ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: train loss 2.21288
wandb:   val loss 2.21257
wandb: 
wandb: üöÄ View run honest-glade-133 at: https://wandb.ai/real-lab/mini-grp/runs/b80imqu9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/real-lab/mini-grp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240904_075851-b80imqu9/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
cfg: batch_size: 512
block_size: 32
n_patches: 8
max_iters: 10000
eval_interval: 100
learning_rate: 7.000000000000001e-05
eval_iters: 200
n_embd: 64
r_seed: 1337
n_head: 16
n_blocks: 4
dropout: 0.2
action_bins: 10
image_shape:
- 64
- 64
- 3
dataset: gberseth/mini-bridge-mini64pix
trim: 1000000
error: false

{'batch_size': 512, 'block_size': 32, 'n_patches': 8, 'max_iters': 10000, 'eval_interval': 100, 'learning_rate': 7.000000000000001e-05, 'eval_iters': 200, 'n_embd': 64, 'r_seed': 1337, 'n_head': 16, 'n_blocks': 4, 'dropout': 0.2, 'action_bins': 10, 'image_shape': [64, 64, 3], 'dataset': 'gberseth/mini-bridge-mini64pix', 'trim': 1000000, 'error': False}
Using device:  cuda (Quadro RTX 8000)
Features: {'img': Image(mode=None, decode=True, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'rotation_delta': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'open_gripper': Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), 'goal': Value(dtype='string', id=None), 'goal_img': Image(mode=None, decode=True, id=None)}
action histogram: [0.11111757 0.11111757 0.11110142 0.11111757 0.13293068 0.08928831
 0.11110142 0.11111757 0.11110142 0.11111757]
bin edges:  [0.  0.9 1.8 2.7 3.6 4.5 5.4 6.3 7.2 8.1 9. ]
Dataset shape: 68766
0.21217 M parameters
step 0: train loss 2.3037, val loss 2.3036
step 100: train loss 2.2988, val loss 2.2990
step 200: train loss 2.2966, val loss 2.2966
step 300: train loss 2.2952, val loss 2.2951
step 400: train loss 2.2936, val loss 2.2937
step 500: train loss 2.2938, val loss 2.2940
step 600: train loss 2.2934, val loss 2.2932
step 700: train loss 2.2922, val loss 2.2923
step 800: train loss 2.2910, val loss 2.2909
step 900: train loss 2.2899, val loss 2.2901
step 1000: train loss 2.2909, val loss 2.2919
step 1100: train loss 2.2873, val loss 2.2874
step 1200: train loss 2.2872, val loss 2.2872
step 1300: train loss 2.2815, val loss 2.2817
step 1400: train loss 2.2808, val loss 2.2816
step 1500: train loss 2.2803, val loss 2.2802
step 1600: train loss 2.2755, val loss 2.2754
step 1700: train loss 2.2761, val loss 2.2761
step 1800: train loss 2.2724, val loss 2.2726
step 1900: train loss 2.2703, val loss 2.2717
step 2000: train loss 2.2714, val loss 2.2702
step 2100: train loss 2.2705, val loss 2.2713
step 2200: train loss 2.2670, val loss 2.2689
step 2300: train loss 2.2667, val loss 2.2675
step 2400: train loss 2.2649, val loss 2.2670
step 2500: train loss 2.2648, val loss 2.2659
step 2600: train loss 2.2621, val loss 2.2615
step 2700: train loss 2.2619, val loss 2.2639
step 2800: train loss 2.2584, val loss 2.2578
step 2900: train loss 2.2564, val loss 2.2567
step 3000: train loss 2.2596, val loss 2.2581
step 3100: train loss 2.2511, val loss 2.2525
step 3200: train loss 2.2491, val loss 2.2497
step 3300: train loss 2.2520, val loss 2.2494
step 3400: train loss 2.2488, val loss 2.2459
step 3500: train loss 2.2438, val loss 2.2437
step 3600: train loss 2.2399, val loss 2.2398
step 3700: train loss 2.2441, val loss 2.2444
step 3800: train loss 2.2389, val loss 2.2405
step 3900: train loss 2.2391, val loss 2.2401
step 4000: train loss 2.2386, val loss 2.2370
step 4100: train loss 2.2366, val loss 2.2374
step 4200: train loss 2.2342, val loss 2.2354
step 4300: train loss 2.2358, val loss 2.2360
step 4400: train loss 2.2344, val loss 2.2360
step 4500: train loss 2.2335, val loss 2.2321
step 4600: train loss 2.2402, val loss 2.2390
step 4700: train loss 2.2312, val loss 2.2319
step 4800: train loss 2.2588, val loss 2.2594
step 4900: train loss 2.2310, val loss 2.2288
step 5000: train loss 2.2297, val loss 2.2297
step 5100: train loss 2.2372, val loss 2.2373
step 5200: train loss 2.2314, val loss 2.2316
step 5300: train loss 2.2304, val loss 2.2332
step 5400: train loss 2.2308, val loss 2.2301
step 5500: train loss 2.2275, val loss 2.2277
step 5600: train loss 2.2303, val loss 2.2312
step 5700: train loss 2.2287, val loss 2.2294
step 5800: train loss 2.2341, val loss 2.2347
step 5900: train loss 2.2240, val loss 2.2241
step 6000: train loss 2.2247, val loss 2.2227
step 6100: train loss 2.2367, val loss 2.2350
step 6200: train loss 2.2278, val loss 2.2272
step 6300: train loss 2.2214, val loss 2.2239
step 6400: train loss 2.2215, val loss 2.2235
step 6500: train loss 2.2227, val loss 2.2222
step 6600: train loss 2.2211, val loss 2.2210
step 6700: train loss 2.2248, val loss 2.2261
step 6800: train loss 2.2206, val loss 2.2181
step 6900: train loss 2.2186, val loss 2.2199
step 7000: train loss 2.2214, val loss 2.2214
step 7100: train loss 2.2185, val loss 2.2190
step 7200: train loss 2.2193, val loss 2.2185
step 7300: train loss 2.2180, val loss 2.2164
step 7400: train loss 2.2169, val loss 2.2197
step 7500: train loss 2.2231, val loss 2.2231
step 7600: train loss 2.2205, val loss 2.2220
step 7700: train loss 2.2222, val loss 2.2236
step 7800: train loss 2.2207, val loss 2.2210
step 7900: train loss 2.2167, val loss 2.2172
step 8000: train loss 2.2175, val loss 2.2176
step 8100: train loss 2.2152, val loss 2.2174
step 8200: train loss 2.2149, val loss 2.2174
step 8300: train loss 2.2173, val loss 2.2163
step 8400: train loss 2.2213, val loss 2.2203
step 8500: train loss 2.2145, val loss 2.2179
step 8600: train loss 2.2153, val loss 2.2143
step 8700: train loss 2.2148, val loss 2.2142
step 8800: train loss 2.2140, val loss 2.2128
step 8900: train loss 2.2131, val loss 2.2148
step 9000: train loss 2.2137, val loss 2.2159
step 9100: train loss 2.2133, val loss 2.2136
step 9200: train loss 2.2163, val loss 2.2158
step 9300: train loss 2.2143, val loss 2.2139
step 9400: train loss 2.2269, val loss 2.2266
step 9500: train loss 2.2147, val loss 2.2131
step 9600: train loss 2.2136, val loss 2.2127
step 9700: train loss 2.2118, val loss 2.2107
step 9800: train loss 2.2161, val loss 2.2147
step 9900: train loss 2.2113, val loss 2.2123
step 9999: train loss 2.2129, val loss 2.2126
[2024-09-04 09:18:16,545][HYDRA] 	#7 : learning_rate=1e-05 dropout=0.0 n_embd=128 batch_size=512
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /home/mila/g/glen.berseth/playground/octo-mini/multirun/2024-09-03/16-32-30/7/wandb/run-20240904_091900-xzoovq7e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-glade-134
wandb: ‚≠êÔ∏è View project at https://wandb.ai/real-lab/mini-grp
wandb: üöÄ View run at https://wandb.ai/real-lab/mini-grp/runs/xzoovq7e
wandb: WARNING No relevant files were detected in the specified directory. No code will be logged to your run.
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(data["img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_goal = torch.tensor(data["goal_img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(data["action"][ix], dtype=torch.long)
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.008 MB uploadedwandb: | 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: train loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: train loss 2.24443
wandb:   val loss 2.24438
wandb: 
wandb: üöÄ View run classic-glade-134 at: https://wandb.ai/real-lab/mini-grp/runs/xzoovq7e
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/real-lab/mini-grp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240904_091900-xzoovq7e/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
cfg: batch_size: 512
block_size: 32
n_patches: 8
max_iters: 10000
eval_interval: 100
learning_rate: 1.0e-05
eval_iters: 200
n_embd: 128
r_seed: 1337
n_head: 16
n_blocks: 4
dropout: 0.0
action_bins: 10
image_shape:
- 64
- 64
- 3
dataset: gberseth/mini-bridge-mini64pix
trim: 1000000
error: false

{'batch_size': 512, 'block_size': 32, 'n_patches': 8, 'max_iters': 10000, 'eval_interval': 100, 'learning_rate': 1e-05, 'eval_iters': 200, 'n_embd': 128, 'r_seed': 1337, 'n_head': 16, 'n_blocks': 4, 'dropout': 0.0, 'action_bins': 10, 'image_shape': [64, 64, 3], 'dataset': 'gberseth/mini-bridge-mini64pix', 'trim': 1000000, 'error': False}
Using device:  cuda (Quadro RTX 8000)
Features: {'img': Image(mode=None, decode=True, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'rotation_delta': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'open_gripper': Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), 'goal': Value(dtype='string', id=None), 'goal_img': Image(mode=None, decode=True, id=None)}
action histogram: [0.11111757 0.11111757 0.11110142 0.11111757 0.13293068 0.08928831
 0.11110142 0.11111757 0.11110142 0.11111757]
bin edges:  [0.  0.9 1.8 2.7 3.6 4.5 5.4 6.3 7.2 8.1 9. ]
Dataset shape: 68766
0.817546 M parameters
step 0: train loss 2.3014, val loss 2.3011
step 100: train loss 2.2992, val loss 2.2993
step 200: train loss 2.2986, val loss 2.2986
step 300: train loss 2.2980, val loss 2.2985
step 400: train loss 2.2979, val loss 2.2980
step 500: train loss 2.2969, val loss 2.2973
step 600: train loss 2.2966, val loss 2.2963
step 700: train loss 2.2952, val loss 2.2955
step 800: train loss 2.2952, val loss 2.2948
step 900: train loss 2.2944, val loss 2.2950
step 1000: train loss 2.2939, val loss 2.2945
step 1100: train loss 2.2943, val loss 2.2934
step 1200: train loss 2.2938, val loss 2.2933
step 1300: train loss 2.2933, val loss 2.2936
step 1400: train loss 2.2932, val loss 2.2936
step 1500: train loss 2.2925, val loss 2.2925
step 1600: train loss 2.2929, val loss 2.2931
step 1700: train loss 2.2926, val loss 2.2921
step 1800: train loss 2.2916, val loss 2.2915
step 1900: train loss 2.2914, val loss 2.2915
step 2000: train loss 2.2907, val loss 2.2909
step 2100: train loss 2.2901, val loss 2.2901
step 2200: train loss 2.2893, val loss 2.2890
step 2300: train loss 2.2888, val loss 2.2883
step 2400: train loss 2.2869, val loss 2.2878
step 2500: train loss 2.2861, val loss 2.2859
step 2600: train loss 2.2839, val loss 2.2837
step 2700: train loss 2.2836, val loss 2.2832
step 2800: train loss 2.2814, val loss 2.2814
step 2900: train loss 2.2804, val loss 2.2794
step 3000: train loss 2.2790, val loss 2.2788
step 3100: train loss 2.2787, val loss 2.2798
step 3200: train loss 2.2786, val loss 2.2785
step 3300: train loss 2.2770, val loss 2.2779
step 3400: train loss 2.2755, val loss 2.2756
step 3500: train loss 2.2743, val loss 2.2760
step 3600: train loss 2.2757, val loss 2.2751
step 3700: train loss 2.2757, val loss 2.2745
step 3800: train loss 2.2736, val loss 2.2733
step 3900: train loss 2.2718, val loss 2.2733
step 4000: train loss 2.2724, val loss 2.2726
step 4100: train loss 2.2706, val loss 2.2715
step 4200: train loss 2.2697, val loss 2.2711
step 4300: train loss 2.2707, val loss 2.2695
step 4400: train loss 2.2732, val loss 2.2732
step 4500: train loss 2.2686, val loss 2.2678
step 4600: train loss 2.2676, val loss 2.2691
step 4700: train loss 2.2684, val loss 2.2666
step 4800: train loss 2.2666, val loss 2.2661
step 4900: train loss 2.2663, val loss 2.2666
step 5000: train loss 2.2649, val loss 2.2670
step 5100: train loss 2.2659, val loss 2.2661
step 5200: train loss 2.2661, val loss 2.2669
step 5300: train loss 2.2629, val loss 2.2653
step 5400: train loss 2.2631, val loss 2.2630
step 5500: train loss 2.2625, val loss 2.2631
step 5600: train loss 2.2609, val loss 2.2618
step 5700: train loss 2.2614, val loss 2.2614
step 5800: train loss 2.2633, val loss 2.2614
step 5900: train loss 2.2616, val loss 2.2599
step 6000: train loss 2.2608, val loss 2.2601
step 6100: train loss 2.2583, val loss 2.2582
step 6200: train loss 2.2581, val loss 2.2583
step 6300: train loss 2.2588, val loss 2.2594
step 6400: train loss 2.2593, val loss 2.2585
step 6500: train loss 2.2558, val loss 2.2569
step 6600: train loss 2.2601, val loss 2.2586
step 6700: train loss 2.2583, val loss 2.2593
step 6800: train loss 2.2546, val loss 2.2544
step 6900: train loss 2.2546, val loss 2.2543
step 7000: train loss 2.2552, val loss 2.2541
step 7100: train loss 2.2564, val loss 2.2560
step 7200: train loss 2.2532, val loss 2.2541
step 7300: train loss 2.2555, val loss 2.2558
step 7400: train loss 2.2509, val loss 2.2513
step 7500: train loss 2.2504, val loss 2.2500
step 7600: train loss 2.2506, val loss 2.2499
step 7700: train loss 2.2491, val loss 2.2495
step 7800: train loss 2.2513, val loss 2.2520
step 7900: train loss 2.2516, val loss 2.2540
step 8000: train loss 2.2520, val loss 2.2537
step 8100: train loss 2.2484, val loss 2.2510
step 8200: train loss 2.2464, val loss 2.2469
step 8300: train loss 2.2465, val loss 2.2473
step 8400: train loss 2.2472, val loss 2.2467
step 8500: train loss 2.2494, val loss 2.2481
step 8600: train loss 2.2485, val loss 2.2483
step 8700: train loss 2.2475, val loss 2.2494
step 8800: train loss 2.2455, val loss 2.2451
step 8900: train loss 2.2453, val loss 2.2441
step 9000: train loss 2.2466, val loss 2.2467
step 9100: train loss 2.2445, val loss 2.2463
step 9200: train loss 2.2427, val loss 2.2424
step 9300: train loss 2.2439, val loss 2.2444
step 9400: train loss 2.2459, val loss 2.2459
step 9500: train loss 2.2461, val loss 2.2465
step 9600: train loss 2.2429, val loss 2.2418
step 9700: train loss 2.2429, val loss 2.2414
step 9800: train loss 2.2417, val loss 2.2424
step 9900: train loss 2.2394, val loss 2.2392
step 9999: train loss 2.2444, val loss 2.2444
[2024-09-04 10:56:21,908][HYDRA] Launching 2 jobs locally
[2024-09-04 10:56:21,908][HYDRA] 	#8 : learning_rate=3.0000000000000004e-05 dropout=0.0 n_embd=64 batch_size=512
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /home/mila/g/glen.berseth/playground/octo-mini/multirun/2024-09-03/16-32-30/8/wandb/run-20240904_105705-h6hlixsz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-glitter-135
wandb: ‚≠êÔ∏è View project at https://wandb.ai/real-lab/mini-grp
wandb: üöÄ View run at https://wandb.ai/real-lab/mini-grp/runs/h6hlixsz
wandb: WARNING No relevant files were detected in the specified directory. No code will be logged to your run.
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(data["img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_goal = torch.tensor(data["goal_img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(data["action"][ix], dtype=torch.long)
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.008 MB uploadedwandb: | 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: train loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: train loss 2.21733
wandb:   val loss 2.21657
wandb: 
wandb: üöÄ View run different-glitter-135 at: https://wandb.ai/real-lab/mini-grp/runs/h6hlixsz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/real-lab/mini-grp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240904_105705-h6hlixsz/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
cfg: batch_size: 512
block_size: 32
n_patches: 8
max_iters: 10000
eval_interval: 100
learning_rate: 3.0000000000000004e-05
eval_iters: 200
n_embd: 64
r_seed: 1337
n_head: 16
n_blocks: 4
dropout: 0.0
action_bins: 10
image_shape:
- 64
- 64
- 3
dataset: gberseth/mini-bridge-mini64pix
trim: 1000000
error: false

{'batch_size': 512, 'block_size': 32, 'n_patches': 8, 'max_iters': 10000, 'eval_interval': 100, 'learning_rate': 3.0000000000000004e-05, 'eval_iters': 200, 'n_embd': 64, 'r_seed': 1337, 'n_head': 16, 'n_blocks': 4, 'dropout': 0.0, 'action_bins': 10, 'image_shape': [64, 64, 3], 'dataset': 'gberseth/mini-bridge-mini64pix', 'trim': 1000000, 'error': False}
Using device:  cuda (Quadro RTX 8000)
Features: {'img': Image(mode=None, decode=True, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'rotation_delta': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'open_gripper': Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), 'goal': Value(dtype='string', id=None), 'goal_img': Image(mode=None, decode=True, id=None)}
action histogram: [0.11111757 0.11111757 0.11110142 0.11111757 0.13293068 0.08928831
 0.11110142 0.11111757 0.11110142 0.11111757]
bin edges:  [0.  0.9 1.8 2.7 3.6 4.5 5.4 6.3 7.2 8.1 9. ]
Dataset shape: 68766
0.21217 M parameters
step 0: train loss 2.3035, val loss 2.3033
step 100: train loss 2.2996, val loss 2.2987
step 200: train loss 2.2984, val loss 2.2981
step 300: train loss 2.2965, val loss 2.2970
step 400: train loss 2.2956, val loss 2.2957
step 500: train loss 2.2945, val loss 2.2945
step 600: train loss 2.2939, val loss 2.2935
step 700: train loss 2.2931, val loss 2.2936
step 800: train loss 2.2931, val loss 2.2938
step 900: train loss 2.2927, val loss 2.2926
step 1000: train loss 2.2926, val loss 2.2922
step 1100: train loss 2.2916, val loss 2.2918
step 1200: train loss 2.2918, val loss 2.2916
step 1300: train loss 2.2906, val loss 2.2904
step 1400: train loss 2.2899, val loss 2.2904
step 1500: train loss 2.2883, val loss 2.2889
step 1600: train loss 2.2887, val loss 2.2884
step 1700: train loss 2.2869, val loss 2.2875
step 1800: train loss 2.2868, val loss 2.2868
step 1900: train loss 2.2857, val loss 2.2849
step 2000: train loss 2.2842, val loss 2.2837
step 2100: train loss 2.2843, val loss 2.2841
step 2200: train loss 2.2802, val loss 2.2802
step 2300: train loss 2.2794, val loss 2.2792
step 2400: train loss 2.2792, val loss 2.2793
step 2500: train loss 2.2781, val loss 2.2782
step 2600: train loss 2.2753, val loss 2.2757
step 2700: train loss 2.2759, val loss 2.2769
step 2800: train loss 2.2786, val loss 2.2790
step 2900: train loss 2.2726, val loss 2.2723
step 3000: train loss 2.2728, val loss 2.2724
step 3100: train loss 2.2717, val loss 2.2707
step 3200: train loss 2.2701, val loss 2.2713
step 3300: train loss 2.2686, val loss 2.2686
step 3400: train loss 2.2675, val loss 2.2682
step 3500: train loss 2.2689, val loss 2.2669
step 3600: train loss 2.2648, val loss 2.2648
step 3700: train loss 2.2654, val loss 2.2655
step 3800: train loss 2.2630, val loss 2.2642
step 3900: train loss 2.2611, val loss 2.2616
step 4000: train loss 2.2589, val loss 2.2581
step 4100: train loss 2.2544, val loss 2.2539
step 4200: train loss 2.2590, val loss 2.2598
step 4300: train loss 2.2513, val loss 2.2490
step 4400: train loss 2.2543, val loss 2.2548
step 4500: train loss 2.2471, val loss 2.2472
step 4600: train loss 2.2460, val loss 2.2454
step 4700: train loss 2.2448, val loss 2.2456
step 4800: train loss 2.2420, val loss 2.2431
step 4900: train loss 2.2433, val loss 2.2419
step 5000: train loss 2.2417, val loss 2.2410
step 5100: train loss 2.2421, val loss 2.2431
step 5200: train loss 2.2403, val loss 2.2421
step 5300: train loss 2.2372, val loss 2.2372
step 5400: train loss 2.2392, val loss 2.2388
step 5500: train loss 2.2365, val loss 2.2378
step 5600: train loss 2.2376, val loss 2.2366
step 5700: train loss 2.2376, val loss 2.2379
step 5800: train loss 2.2335, val loss 2.2358
step 5900: train loss 2.2360, val loss 2.2360
step 6000: train loss 2.2331, val loss 2.2326
step 6100: train loss 2.2441, val loss 2.2447
step 6200: train loss 2.2323, val loss 2.2334
step 6300: train loss 2.2315, val loss 2.2316
step 6400: train loss 2.2361, val loss 2.2361
step 6500: train loss 2.2298, val loss 2.2296
step 6600: train loss 2.2318, val loss 2.2315
step 6700: train loss 2.2284, val loss 2.2269
step 6800: train loss 2.2317, val loss 2.2320
step 6900: train loss 2.2259, val loss 2.2263
step 7000: train loss 2.2280, val loss 2.2259
step 7100: train loss 2.2274, val loss 2.2247
step 7200: train loss 2.2365, val loss 2.2362
step 7300: train loss 2.2251, val loss 2.2235
step 7400: train loss 2.2293, val loss 2.2306
step 7500: train loss 2.2287, val loss 2.2280
step 7600: train loss 2.2273, val loss 2.2261
step 7700: train loss 2.2247, val loss 2.2236
step 7800: train loss 2.2248, val loss 2.2235
step 7900: train loss 2.2222, val loss 2.2208
step 8000: train loss 2.2311, val loss 2.2325
step 8100: train loss 2.2227, val loss 2.2232
step 8200: train loss 2.2212, val loss 2.2224
step 8300: train loss 2.2232, val loss 2.2232
step 8400: train loss 2.2217, val loss 2.2210
step 8500: train loss 2.2229, val loss 2.2218
step 8600: train loss 2.2218, val loss 2.2226
step 8700: train loss 2.2214, val loss 2.2203
step 8800: train loss 2.2219, val loss 2.2214
step 8900: train loss 2.2231, val loss 2.2226
step 9000: train loss 2.2158, val loss 2.2182
step 9100: train loss 2.2194, val loss 2.2198
step 9200: train loss 2.2214, val loss 2.2236
step 9300: train loss 2.2186, val loss 2.2178
step 9400: train loss 2.2194, val loss 2.2193
step 9500: train loss 2.2156, val loss 2.2166
step 9600: train loss 2.2150, val loss 2.2150
step 9700: train loss 2.2158, val loss 2.2157
step 9800: train loss 2.2211, val loss 2.2216
step 9900: train loss 2.2161, val loss 2.2171
step 9999: train loss 2.2173, val loss 2.2166
[2024-09-04 12:12:28,220][HYDRA] 	#9 : learning_rate=7.000000000000001e-05 dropout=0.0 n_embd=128 batch_size=512
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /home/mila/g/glen.berseth/playground/octo-mini/multirun/2024-09-03/16-32-30/9/wandb/run-20240904_121311-2hbotncv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-silence-136
wandb: ‚≠êÔ∏è View project at https://wandb.ai/real-lab/mini-grp
wandb: üöÄ View run at https://wandb.ai/real-lab/mini-grp/runs/2hbotncv
wandb: WARNING No relevant files were detected in the specified directory. No code will be logged to your run.
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(data["img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_goal = torch.tensor(data["goal_img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(data["action"][ix], dtype=torch.long)
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.023 MB uploadedwandb: | 0.008 MB of 0.023 MB uploadedwandb: / 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: train loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: train loss 2.0736
wandb:   val loss 2.07133
wandb: 
wandb: üöÄ View run peach-silence-136 at: https://wandb.ai/real-lab/mini-grp/runs/2hbotncv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/real-lab/mini-grp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240904_121311-2hbotncv/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
cfg: batch_size: 512
block_size: 32
n_patches: 8
max_iters: 10000
eval_interval: 100
learning_rate: 7.000000000000001e-05
eval_iters: 200
n_embd: 128
r_seed: 1337
n_head: 16
n_blocks: 4
dropout: 0.0
action_bins: 10
image_shape:
- 64
- 64
- 3
dataset: gberseth/mini-bridge-mini64pix
trim: 1000000
error: false

{'batch_size': 512, 'block_size': 32, 'n_patches': 8, 'max_iters': 10000, 'eval_interval': 100, 'learning_rate': 7.000000000000001e-05, 'eval_iters': 200, 'n_embd': 128, 'r_seed': 1337, 'n_head': 16, 'n_blocks': 4, 'dropout': 0.0, 'action_bins': 10, 'image_shape': [64, 64, 3], 'dataset': 'gberseth/mini-bridge-mini64pix', 'trim': 1000000, 'error': False}
Using device:  cuda (Quadro RTX 8000)
Features: {'img': Image(mode=None, decode=True, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'rotation_delta': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'open_gripper': Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), 'goal': Value(dtype='string', id=None), 'goal_img': Image(mode=None, decode=True, id=None)}
action histogram: [0.11111757 0.11111757 0.11110142 0.11111757 0.13293068 0.08928831
 0.11110142 0.11111757 0.11110142 0.11111757]
bin edges:  [0.  0.9 1.8 2.7 3.6 4.5 5.4 6.3 7.2 8.1 9. ]
Dataset shape: 68766
0.817546 M parameters
step 0: train loss 2.3012, val loss 2.3011
step 100: train loss 2.2974, val loss 2.2973
step 200: train loss 2.2945, val loss 2.2944
step 300: train loss 2.2934, val loss 2.2936
step 400: train loss 2.2885, val loss 2.2889
step 500: train loss 2.2814, val loss 2.2810
step 600: train loss 2.2780, val loss 2.2771
step 700: train loss 2.2751, val loss 2.2761
step 800: train loss 2.2711, val loss 2.2704
step 900: train loss 2.2683, val loss 2.2700
step 1000: train loss 2.2655, val loss 2.2639
step 1100: train loss 2.2649, val loss 2.2648
step 1200: train loss 2.2641, val loss 2.2622
step 1300: train loss 2.2602, val loss 2.2608
step 1400: train loss 2.2611, val loss 2.2597
step 1500: train loss 2.2544, val loss 2.2541
step 1600: train loss 2.2520, val loss 2.2532
step 1700: train loss 2.2480, val loss 2.2487
step 1800: train loss 2.2424, val loss 2.2430
step 1900: train loss 2.2388, val loss 2.2407
step 2000: train loss 2.2425, val loss 2.2424
step 2100: train loss 2.2274, val loss 2.2292
step 2200: train loss 2.2262, val loss 2.2259
step 2300: train loss 2.2256, val loss 2.2262
step 2400: train loss 2.2223, val loss 2.2194
step 2500: train loss 2.2153, val loss 2.2163
step 2600: train loss 2.2116, val loss 2.2135
step 2700: train loss 2.2127, val loss 2.2141
step 2800: train loss 2.2138, val loss 2.2138
step 2900: train loss 2.2115, val loss 2.2146
step 3000: train loss 2.2052, val loss 2.2038
step 3100: train loss 2.2046, val loss 2.2045
step 3200: train loss 2.2052, val loss 2.2030
step 3300: train loss 2.2062, val loss 2.2056
step 3400: train loss 2.1955, val loss 2.1950
step 3500: train loss 2.1965, val loss 2.1970
step 3600: train loss 2.1901, val loss 2.1913
step 3700: train loss 2.1923, val loss 2.1927
step 3800: train loss 2.2079, val loss 2.2074
step 3900: train loss 2.1827, val loss 2.1834
step 4000: train loss 2.1895, val loss 2.1856
step 4100: train loss 2.1827, val loss 2.1797
step 4200: train loss 2.1858, val loss 2.1851
step 4300: train loss 2.1794, val loss 2.1820
step 4400: train loss 2.1779, val loss 2.1772
step 4500: train loss 2.1790, val loss 2.1759
step 4600: train loss 2.1784, val loss 2.1812
step 4700: train loss 2.1793, val loss 2.1827
step 4800: train loss 2.1698, val loss 2.1685
step 4900: train loss 2.1702, val loss 2.1743
step 5000: train loss 2.1656, val loss 2.1634
step 5100: train loss 2.1645, val loss 2.1637
step 5200: train loss 2.1602, val loss 2.1592
step 5300: train loss 2.1631, val loss 2.1629
step 5400: train loss 2.1544, val loss 2.1548
step 5500: train loss 2.1554, val loss 2.1552
step 5600: train loss 2.1528, val loss 2.1538
step 5700: train loss 2.1512, val loss 2.1506
step 5800: train loss 2.1460, val loss 2.1498
step 5900: train loss 2.1533, val loss 2.1540
step 6000: train loss 2.1451, val loss 2.1453
step 6100: train loss 2.1412, val loss 2.1430
step 6200: train loss 2.1380, val loss 2.1399
step 6300: train loss 2.1402, val loss 2.1402
step 6400: train loss 2.1357, val loss 2.1353
step 6500: train loss 2.1341, val loss 2.1336
step 6600: train loss 2.1346, val loss 2.1332
step 6700: train loss 2.1308, val loss 2.1307
step 6800: train loss 2.1329, val loss 2.1319
step 6900: train loss 2.1378, val loss 2.1367
step 7000: train loss 2.1275, val loss 2.1296
step 7100: train loss 2.1231, val loss 2.1216
step 7200: train loss 2.1271, val loss 2.1257
step 7300: train loss 2.1158, val loss 2.1153
step 7400: train loss 2.1210, val loss 2.1216
step 7500: train loss 2.1233, val loss 2.1218
step 7600: train loss 2.1239, val loss 2.1270
step 7700: train loss 2.1113, val loss 2.1098
step 7800: train loss 2.1151, val loss 2.1125
step 7900: train loss 2.1026, val loss 2.1025
step 8000: train loss 2.1050, val loss 2.1097
step 8100: train loss 2.1106, val loss 2.1088
step 8200: train loss 2.1052, val loss 2.1057
step 8300: train loss 2.1002, val loss 2.1019
step 8400: train loss 2.1014, val loss 2.0999
step 8500: train loss 2.0980, val loss 2.1018
step 8600: train loss 2.0973, val loss 2.0965
step 8700: train loss 2.1037, val loss 2.1019
step 8800: train loss 2.0933, val loss 2.0922
step 8900: train loss 2.0934, val loss 2.0942
step 9000: train loss 2.0829, val loss 2.0844
step 9100: train loss 2.0853, val loss 2.0877
step 9200: train loss 2.0887, val loss 2.0894
step 9300: train loss 2.0830, val loss 2.0830
step 9400: train loss 2.0735, val loss 2.0749
step 9500: train loss 2.0869, val loss 2.0891
step 9600: train loss 2.0724, val loss 2.0712
step 9700: train loss 2.0792, val loss 2.0769
step 9800: train loss 2.0777, val loss 2.0789
step 9900: train loss 2.0747, val loss 2.0771
step 9999: train loss 2.0736, val loss 2.0713
[2024-09-04 13:50:25,556][HYDRA] Launching 2 jobs locally
[2024-09-04 13:50:25,556][HYDRA] 	#10 : learning_rate=9e-05 dropout=0.1 n_embd=512 batch_size=256
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.7
wandb: Run data is saved locally in /home/mila/g/glen.berseth/playground/octo-mini/multirun/2024-09-03/16-32-30/10/wandb/run-20240904_135109-wynfsyg2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-aardvark-137
wandb: ‚≠êÔ∏è View project at https://wandb.ai/real-lab/mini-grp
wandb: üöÄ View run at https://wandb.ai/real-lab/mini-grp/runs/wynfsyg2
wandb: WARNING No relevant files were detected in the specified directory. No code will be logged to your run.
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(data["img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_goal = torch.tensor(data["goal_img"][ix], dtype=torch.float)
/home/mila/g/glen.berseth/playground/octo-mini/vit-plus-goalIMG-64.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(data["action"][ix], dtype=torch.long)
slurmstepd: error: *** JOB 5252457 ON cn-c015 CANCELLED AT 2024-09-04T16:31:29 DUE TO TIME LIMIT ***

======== GPU REPORT ========

==============NVSMI LOG==============

Timestamp                                 : Wed Sep  4 16:31:29 2024
Driver Version                            : 535.183.06
CUDA Version                              : 12.2

Attached GPUs                             : 1
GPU 00000000:28:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 473328
            GPU Utilization               : 98 %
            Memory Utilization            : 85 %
            Max memory usage              : 20598 MiB
            Time                          : 0 ms
            Is Running                    : 1

Wed Sep  4 16:31:29 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro RTX 8000                On  | 00000000:28:00.0 Off |                    0 |
| 48%   70C    P2             258W / 260W |  20601MiB / 46080MiB |    100%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A    473328      C   python                                    20598MiB |
+---------------------------------------------------------------------------------------+
